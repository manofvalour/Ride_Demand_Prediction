{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90ee5e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/Dynamic-Pricing-Engine'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5ba659b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "313cf088",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbg = XGBRegressor(n_job=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0321036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2026-01-10'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import timedelta, datetime\n",
    "#extracting the date\n",
    "datetime.today().strftime('%Y-%m-%d').\n",
    "#datetime.today()# - relativedelta(months=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a327f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna-integration in /usr/local/python/3.12.1/lib/python3.12/site-packages (4.6.0)\n",
      "Requirement already satisfied: optuna in /home/codespace/.local/lib/python3.12/site-packages (from optuna-integration) (4.6.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from optuna->optuna-integration) (1.17.2)\n",
      "Requirement already satisfied: colorlog in /home/codespace/.local/lib/python3.12/site-packages (from optuna->optuna-integration) (6.10.1)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.12/site-packages (from optuna->optuna-integration) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from optuna->optuna-integration) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /home/codespace/.local/lib/python3.12/site-packages (from optuna->optuna-integration) (2.0.29)\n",
      "Requirement already satisfied: tqdm in /home/codespace/.local/lib/python3.12/site-packages (from optuna->optuna-integration) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /home/codespace/.local/lib/python3.12/site-packages (from optuna->optuna-integration) (6.0.3)\n",
      "Requirement already satisfied: Mako in /home/codespace/.local/lib/python3.12/site-packages (from alembic>=1.5.0->optuna->optuna-integration) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /home/codespace/.local/lib/python3.12/site-packages (from alembic>=1.5.0->optuna->optuna-integration) (4.15.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from sqlalchemy>=1.4.2->optuna->optuna-integration) (3.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/codespace/.local/lib/python3.12/site-packages (from Mako->alembic>=1.5.0->optuna->optuna-integration) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f58e64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.DynamicPricingEngine.utils.data_ingestion_utils import time_subtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a20a3d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start date: 2025-10-01, end date: 2025-10-31\n"
     ]
    }
   ],
   "source": [
    "## two months into the past\n",
    "now = (datetime.today()- relativedelta(months=1))\n",
    "# end_date = datetime.strptime(now, \"%Y-%m-%d\") ## converting to datetime\n",
    "end_date = now - timedelta(days=now.day) ## retrieving the last day of the previous month\n",
    "\n",
    "## accessing the previous month\n",
    "days_to_subtract = time_subtract(end_date.strftime('%Y-%m-%d'))\n",
    "end_date = (end_date - timedelta(days=days_to_subtract))\n",
    "\n",
    "## start of the month\n",
    "days= time_subtract(end_date.strftime('%Y-%m-%d'))\n",
    "start_date= end_date - timedelta(days=days-1)\n",
    "\n",
    "#self.config = config\n",
    "start_date = start_date.strftime('%Y-%m-%d')\n",
    "end_date = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "print(f'start date: {start_date}, end date: {end_date}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b78063",
   "metadata": {},
   "source": [
    "## Exploratory Data Anaysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30685d8",
   "metadata": {},
   "source": [
    "### Things to do:\n",
    "- Data Collection\n",
    "- Data Checks\n",
    "- Exploratory Data Analysis\n",
    "- Data Preproccessing and Feature Engineering\n",
    "- Model Training, Evaluation, and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4473780",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_html = pd.read_html('https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)',match = 'by country')\n",
    "# Let's see how many tables are there with tage ' by county'\n",
    "print(len(df_html)) # There are 4 tables\n",
    "# Let's see the first table\n",
    "df_html[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa5b70b",
   "metadata": {},
   "source": [
    "### Extracting Weather Information for NYC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b7fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "$JAVA_HOME/bin/java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73f517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('dynamicpricing').getOrCreate()\n",
    "#spark = (\n",
    " #   SparkSession.builder\n",
    "  #      .master(\"local\")\n",
    "   #     .appName(\"dynamicPricing\")\n",
    "    #    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a7afe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['JAVA_HOME']= "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55129de",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SPARK_HOME']= 'opt/apache-spark/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22da3ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85c6a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import dask.dataframe as dd\n",
    "from datetime import datetime, timedelta\n",
    "import hopsworks\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from src.DynamicPricingEngine.logger.logger import logger\n",
    "from src.DynamicPricingEngine.exception.customexception import RideDemandException\n",
    "from src.DynamicPricingEngine.entity.config_entity import DataTransformationConfig\n",
    "from src.DynamicPricingEngine.utils.common_utils import load_shapefile_from_zip\n",
    "from src.DynamicPricingEngine.utils.data_ingestion_utils import time_subtract\n",
    "\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig, \n",
    "                 nyc_taxi_data: str, \n",
    "                 nyc_weather_data: str\n",
    "                 ):\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        #Read both datasets with Dask\n",
    "        self.taxi_df  =dd.read_parquet(nyc_taxi_data, engine='pyarrow')\n",
    "        self.weather_df =dd.read_csv(nyc_weather_data)\n",
    "\n",
    "        self.taxi_df.index = self.taxi_df.index.astype('int32')\n",
    "        self.weather_df.index = self.weather_df.index.astype('int32')\n",
    "\n",
    "        #Ensure datetime types\n",
    "        for col in ['tpep_pickup_datetime', 'tpep_dropoff_datetime']:\n",
    "            if col in self.taxi_df.columns:\n",
    "                self.taxi_df[col] = dd.to_datetime(self.taxi_df[col], errors='coerce')\n",
    "\n",
    "        # Precompute bin\n",
    "        if 'tpep_pickup_datetime' in self.taxi_df.columns:\n",
    "            self.taxi_df['bin'] = self.taxi_df['tpep_pickup_datetime'].dt.floor('60min')\n",
    "\n",
    "         #Cache neighbor dictionary\n",
    "        self._neighbor_dict = None\n",
    "        self._neighbor_cache_path = os.path.join(self.config.shapefile_dir, \"neighbors.pkl\")\n",
    "\n",
    "    def _get_neighbor_dict(self) -> dict:\n",
    "        if self._neighbor_dict is not None:\n",
    "            return self._neighbor_dict\n",
    "\n",
    "        if os.path.exists(self._neighbor_cache_path):\n",
    "            try:\n",
    "                with open(self._neighbor_cache_path, \"rb\") as f:\n",
    "                    self._neighbor_dict = pickle.load(f)\n",
    "                logger.info(\"Loaded neighbor dictionary from cache\")\n",
    "                return self._neighbor_dict\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to load neighbor cache: {e}\")\n",
    "\n",
    "        zones_gdf = load_shapefile_from_zip(self.config.taxi_zone_shapefile_url, \n",
    "                                            self.config.shapefile_dir)\n",
    "        zones_gdf_left = zones_gdf.rename(columns={\"LocationID\": \"LocationID_left\"})\n",
    "        zones_gdf_right = zones_gdf.rename(columns={\"LocationID\": \"LocationID_right\"})\n",
    "        neighbors_df = gpd.sjoin(zones_gdf_left, zones_gdf_right, how=\"left\", predicate=\"touches\")\n",
    "        neighbors_df = neighbors_df[neighbors_df['LocationID_left'] != neighbors_df['LocationID_right']]\n",
    "        self._neighbor_dict = (neighbors_df.groupby('LocationID_left')['LocationID_right']\n",
    "                               .apply(lambda s: sorted(list(set(s))))\n",
    "                               .to_dict())\n",
    "        try:\n",
    "            os.makedirs(self.config.shapefile_dir, exist_ok=True)\n",
    "            with open(self._neighbor_cache_path, \"wb\") as f:\n",
    "                pickle.dump(self._neighbor_dict, f)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to persist neighbor cache: {e}\")\n",
    "        return self._neighbor_dict\n",
    "\n",
    "\n",
    "    def derive_target_and_join_to_weather_feature(self) -> dd.DataFrame:\n",
    "        try:\n",
    "            taxi_df = self.taxi_df[['PULocationID', 'bin']]\n",
    "\n",
    "            # Aggregate pickups per zone-hour\n",
    "            y = (taxi_df\n",
    "                 .groupby(['PULocationID', 'bin'])\n",
    "                 .size().rename('pickups')\n",
    "                 .reset_index())\n",
    "            \n",
    "            y['bin']= y['bin'].astype('datetime64[ns]')\n",
    "            y['PULocationID']= y['PULocationID'].astype('int32')\n",
    "\n",
    "            # Materialized data to Pandas to build full grid then back to Dask\n",
    "            zones = y['PULocationID'].unique().compute()\n",
    "\n",
    "            time_index = pd.date_range(y['bin'].min().compute(), \n",
    "                                       y['bin'].max().compute(), \n",
    "                                       freq='60min')\n",
    "            \n",
    "            grid = pd.MultiIndex.from_product([zones, time_index], \n",
    "                                              names=['PULocationID', \n",
    "                                                     'bin']).to_frame(index=False)\n",
    "\n",
    "            #y['PULocationID'] = y['PULocationID'].astype('int32')\n",
    "            grid['PULocationID'] = grid['PULocationID'].astype('int32')\n",
    "\n",
    "            # Align datetime precision\n",
    "            y['bin'] = y['bin'].astype('datetime64[ns]')\n",
    "            grid['bin'] = grid['bin'].astype('datetime64[ns]')\n",
    "\n",
    "            y = dd.from_pandas(grid, npartitions=4).merge(y, how='left', \n",
    "                                                          on=['PULocationID', 'bin'])\n",
    "            y = y.fillna({'pickups': 0})\n",
    "\n",
    "            # Weather alignment\n",
    "            weather_df = self.weather_df\n",
    "            weather_df['bin'] = dd.to_datetime(\n",
    "                weather_df['day'].astype(str) + ' ' + \n",
    "                weather_df['datetime'].astype(str),\n",
    "                errors='coerce'\n",
    "            )\n",
    "            \n",
    "            #Dropping the Day column\n",
    "            weather_df = weather_df.drop(columns='day')\n",
    "            y.index = y.index.astype('int32')\n",
    "\n",
    "            # Merge target + weather and sort by PUlocationID and bin\n",
    "            df = y.merge(weather_df, on='bin', how='left').map_partitions(\n",
    "                lambda pdf: pdf.sort_values(['PULocationID', 'bin'])\n",
    "            )\n",
    "\n",
    "            return (df)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Failed to generate the target feature\", exc_info=True)\n",
    "            raise RideDemandException(e, sys)\n",
    "        \n",
    "    def engineer_temporal_feature(self, df: dd.DataFrame) -> dd.DataFrame:\n",
    "        \"\"\"\n",
    "        Deriving the temporal feature\n",
    "        \n",
    "        :type df: dd.DataFrame\n",
    "        :return: Description\n",
    "        :rtype: DataFrame\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Temporal from 'bin'\n",
    "            df['pickup_year'] = df['bin'].dt.year\n",
    "            df['pickup_month'] = df['bin'].dt.month\n",
    "            df['day_of_month'] = df['bin'].dt.day\n",
    "            df['Pickup_hour'] = df['bin'].dt.hour\n",
    "            df['day_of_week'] = df['bin'].dt.dayofweek\n",
    "            df[\"bin_str\"] = df[\"bin\"].astype('str')\n",
    "\n",
    "            # Vectorized flags\n",
    "            df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype('int8')\n",
    "            df['is_rush_hour'] = df['Pickup_hour'].isin([7, 8, 9, 16, 17, 18, 19]).astype('int8')\n",
    "            df['is_night_hour'] = df['Pickup_hour'].isin([0,1,2,3,4,5,6,20,21,22,23]).astype('int8')\n",
    "\n",
    "            # Season mapping\n",
    "            season_map = {\n",
    "                12: 'winter', 1: 'winter', 2: 'winter',\n",
    "                3: 'spring', 4: 'spring', 5: 'spring',\n",
    "                6: 'summer', 7: 'summer', 8: 'summer',\n",
    "                9: 'autumn', 10: 'autumn', 11: 'autumn'\n",
    "            }\n",
    "            df['season_of_year'] = df['pickup_month'].map(season_map)\n",
    "\n",
    "            # Fixed holidays and specials\n",
    "            fixed_holidays = {(1, 1), (7, 4), (11, 11), (6, 19), (12, 25)}\n",
    "            fixed_specials = {(3, 17), (7, 4), (6, 4), (12, 31),\n",
    "                            (6, 5), (6, 6), (6, 7), (6, 8), (6, 9), (6, 10),\n",
    "                            (6, 11), (6, 12), (6, 13), (6, 14), (6, 15)}\n",
    "\n",
    "            df['is_holiday'] = df[['pickup_month', 'day_of_month']].map_partitions(\n",
    "                lambda pdf: pdf.apply(lambda r: int((r['pickup_month'], \n",
    "                                                     r['day_of_month']) in fixed_holidays), axis=1),\n",
    "                meta=('is_holiday', 'int8')\n",
    "            )\n",
    "\n",
    "            df['Is_special_event'] = df[['pickup_month', 'day_of_month']].map_partitions(\n",
    "                lambda pdf: pdf.apply(lambda r: int((r['pickup_month'], \n",
    "                                                     r['day_of_month']) in fixed_specials), axis=1),\n",
    "                meta=('Is_special_event', 'int8')\n",
    "            )\n",
    "\n",
    "            # Movable holidays and specials (row-wise logic via map_partitions)\n",
    "            def add_movable_flags(pdf: dd.DataFrame) -> dd.DataFrame:\n",
    "                def _movable_holiday(row):\n",
    "                    y, m, d = row['pickup_year'], row['pickup_month'], row['day_of_month']\n",
    "                    date = pd.Timestamp(y, m, d)\n",
    "                    if m == 11 and date.weekday() == 3 and date + pd.Timedelta(days=7) > pd.Timestamp(y, 11, 30):\n",
    "                        return 1\n",
    "                    if m == 5 and date.weekday() == 0 and date + pd.Timedelta(days=7) > pd.Timestamp(y, 5, 31):\n",
    "                        return 1\n",
    "                    if m == 1 and date.weekday() == 0 and 15 <= d <= 21:\n",
    "                        return 1\n",
    "                    if m == 9 and date.weekday() == 0 and 1 <= d <= 7:\n",
    "                        return 1\n",
    "                    if m == 11 and date.weekday() == 1 and 1 <= d <= 7:\n",
    "                        return 1\n",
    "                    if m == 10 and date.weekday() == 0 and 8 <= d <= 14:\n",
    "                        return 1\n",
    "                    return 0\n",
    "\n",
    "                def _movable_special(row):\n",
    "                    y, m, d = row['pickup_year'], row['pickup_month'], row['day_of_month']\n",
    "                    date = pd.Timestamp(y, m, d)\n",
    "                    if m == 11 and date.weekday() == 3 and date + pd.Timedelta(days=7) > pd.Timestamp(y, 11, 30):\n",
    "                        return 1\n",
    "                    if m == 6 and date.weekday() == 6 and date + pd.Timedelta(days=7) > pd.Timestamp(y, 6, 30):\n",
    "                        return 1\n",
    "                    return 0\n",
    "\n",
    "                pdf['is_holiday'] = pdf['is_holiday'].where(pdf['is_holiday'] == 1,\n",
    "                                                        pdf.apply(_movable_holiday, axis=1))\n",
    "                pdf.loc[pdf['Is_special_event'] == 0, 'Is_special_event'] = pdf.loc[pdf['Is_special_event'] == 0].apply(_movable_special, axis=1)\n",
    "                \n",
    "                return pdf\n",
    "\n",
    "            df = df.map_partitions(add_movable_flags) #meta=df._meta)\n",
    "\n",
    "            ## creating a column for Payday Indicator\n",
    "            def is_payday(data):\n",
    "\n",
    "                date = pd.Timestamp(year=data['pickup_year'], \n",
    "                                    month=data['pickup_month'], \n",
    "                                    day=data['day_of_month'])\n",
    "\n",
    "                if date.is_month_end:\n",
    "                    return 1\n",
    "                \n",
    "                if date.day ==(15 or 16 or 17) and date.isoweekday!=(6 or 7):\n",
    "                    return 1\n",
    "\n",
    "                return 0\n",
    "\n",
    "            #pdf = df.compute()\n",
    "\n",
    "            pdf= df[['PULocationID', 'bin', 'pickup_year', \n",
    "                     'pickup_month', 'day_of_month']].compute()\n",
    "\n",
    "            pdf['is_payday'] = pdf.apply(is_payday, axis=1) ##deriving the payday indicator\n",
    "\n",
    "            df = df.merge(dd.from_pandas(pdf, npartitions=4), \n",
    "                          on=['PULocationID', \"bin\"], how='left')\n",
    "            df= df.rename(columns={'pickup_year_x':'pickup_year', \n",
    "                                   'pickup_month_x':'pickup_month', \n",
    "                                   'day_of_month_x':'day_of_month'})\n",
    "            \n",
    "            df = df.drop(['pickup_year_y', 'pickup_month_y', \n",
    "                          'day_of_month_y'], axis=1)\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Failed to engineer temporal features\", exc_info=True)\n",
    "            raise RideDemandException(e, sys)\n",
    "\n",
    "\n",
    "    def city_wide_congestion_features(self, df: pd.DataFrame) -> dd.DataFrame:\n",
    "        try:\n",
    "            # Select needed columns\n",
    "            taxi_df = self.taxi_df[['bin', 'tpep_pickup_datetime', \n",
    "                                    'tpep_dropoff_datetime', 'trip_distance']]\n",
    "\n",
    "            # Compute trip duration in hours\n",
    "            taxi_df['trip_duration_hr'] = (\n",
    "                (taxi_df['tpep_dropoff_datetime'] - \n",
    "                 taxi_df['tpep_pickup_datetime']).dt.total_seconds() / 3600\n",
    "            )\n",
    "\n",
    "            # Filter invalid trips\n",
    "            taxi_df = taxi_df[(taxi_df['trip_duration_hr'] > 0) & \n",
    "                              (taxi_df['trip_distance'] >= 0)]\n",
    "\n",
    "            # Compute speed\n",
    "            taxi_df['MPH'] = taxi_df['trip_distance'] / taxi_df['trip_duration_hr']\n",
    "            taxi_df = taxi_df[taxi_df['MPH'].between(1, 60)]\n",
    "\n",
    "            # Compute citywide average speed per hour\n",
    "            city_speed = (\n",
    "                taxi_df.groupby('bin')['MPH']\n",
    "                .mean()\n",
    "                .reset_index()\n",
    "                .rename(columns={'MPH': 'city_avg_speed'})\n",
    "            )\n",
    "\n",
    "            # Congestion index\n",
    "            city_speed['city_congestion_index'] = city_speed['city_avg_speed'].map_partitions(\n",
    "                lambda pdf: np.where(pdf > 0, 1.0 / pdf, np.nan),\n",
    "                meta=('city_congestion_index', 'f8')\n",
    "            )\n",
    "\n",
    "            # Merge back into main df\n",
    "            df = df.merge(city_speed, on='bin', how='left')\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Unable to generate city-wide features\", exc_info=True)\n",
    "            raise RideDemandException(e, sys)\n",
    "\n",
    "\n",
    "    def zone_level_congestion_features(self, df: dd.DataFrame) -> dd.DataFrame:\n",
    "        try:\n",
    "            # Select needed columns\n",
    "            taxi_df = self.taxi_df[['PULocationID', 'bin', 'tpep_pickup_datetime',\n",
    "                                    'tpep_dropoff_datetime', 'trip_distance']]\n",
    "\n",
    "            # Compute trip duration in hours\n",
    "            taxi_df['trip_duration_hr'] = (\n",
    "                (taxi_df['tpep_dropoff_datetime'] \n",
    "                 -taxi_df['tpep_pickup_datetime'])\n",
    "                 .dt.total_seconds() / 3600\n",
    "            )\n",
    "\n",
    "            # Filter invalid trips\n",
    "            taxi_df = taxi_df[(taxi_df['trip_duration_hr'] > 0) & (taxi_df['trip_distance'] >= 0)]\n",
    "\n",
    "            # Compute speed\n",
    "            taxi_df['MPH'] = taxi_df['trip_distance'] / taxi_df['trip_duration_hr']\n",
    "            taxi_df = taxi_df[taxi_df['MPH'].between(1, 60)]\n",
    "\n",
    "            # Compute zone-level average speed per hour\n",
    "            zone_speed = (\n",
    "                taxi_df.groupby(['PULocationID', 'bin'])['MPH']\n",
    "                .mean()\n",
    "                .reset_index()\n",
    "                .rename(columns={'MPH': 'zone_avg_speed'})\n",
    "            )\n",
    "\n",
    "            # Congestion index\n",
    "            zone_speed['zone_congestion_index'] = zone_speed['zone_avg_speed'].map_partitions(\n",
    "                lambda pdf: np.where(pdf > 0, 1.0 / pdf, np.nan),\n",
    "                meta=('zone_congestion_index', 'f8')\n",
    "            )\n",
    "\n",
    "            # Build full grid (zones Ã— time) in Pandas, then convert back to Dask\n",
    "            zones = df['PULocationID'].unique().compute()\n",
    "            time_index = pd.date_range(df['bin'].min().compute(), \n",
    "                                       df['bin'].max().compute(), freq='60min'\n",
    "                                       )\n",
    "            \n",
    "            grid = pd.MultiIndex.from_product([zones, time_index], \n",
    "                                              names=['PULocationID', \n",
    "                                                     'bin']).to_frame(index=False\n",
    "                                                                      )\n",
    "            \n",
    "            grid_dd = dd.from_pandas(grid, npartitions=4)\n",
    "\n",
    "            zone_speed['PULocationID']= zone_speed['PULocationID'].astype('int32')\n",
    "\n",
    "            # Merge grid with zone_speed to ensure full coverage\n",
    "            zone_speed = grid_dd.merge(zone_speed, how='left', on=['PULocationID', 'bin'])\n",
    "            zone_speed[['zone_avg_speed', 'zone_congestion_index']] = zone_speed[\n",
    "                ['zone_avg_speed', 'zone_congestion_index']\n",
    "            ].fillna(0)\n",
    "\n",
    "            #zone_speed['PULocationID']= zone_speed['PULocationID'].astype('int32')\n",
    "            # Merge back into main df\n",
    "            df = df.merge(zone_speed, on=['PULocationID', 'bin'], how='left')\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Unable to generate zone-level features\", exc_info=True)\n",
    "            raise RideDemandException(e, sys)\n",
    "\n",
    "\n",
    "    def citywide_hourly_demand(self, df: dd.DataFrame) -> dd.DataFrame:\n",
    "        try:\n",
    "            # Aggregate citywide pickups per hour\n",
    "            city_demand = (\n",
    "                df.groupby('bin')['pickups']\n",
    "                .sum()\n",
    "                .reset_index()\n",
    "                .rename(columns={'pickups': 'city_pickups'})\n",
    "            )\n",
    "\n",
    "            # Merge back into main df\n",
    "            df = df.merge(city_demand, on='bin', how='left')\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Unable to engineer citywide hourly demand features\", exc_info=True)\n",
    "            raise RideDemandException(e, sys)\n",
    "\n",
    "\n",
    "    def generate_neighbor_features(self, df: dd.DataFrame) -> dd.DataFrame:\n",
    "        try:\n",
    "            neighbor_dict = self._get_neighbor_dict()\n",
    "\n",
    "            # Build neighbor pairs in Pandas, then convert to Dask\n",
    "            neighbor_pdf = pd.DataFrame(\n",
    "                [(zone, n) for zone, neighs in neighbor_dict.items() for n in neighs],\n",
    "                columns=['PULocationID', 'neighbor_id']\n",
    "            ).fillna(-1)\n",
    "\n",
    "            neighbor_pdf['PULocationID'] = neighbor_pdf['PULocationID'].astype(\n",
    "                df['PULocationID'].dtype\n",
    "                )\n",
    "            neighbor_pdf['neighbor_id'] = neighbor_pdf['neighbor_id'].astype(\n",
    "                df['PULocationID'].dtype\n",
    "                )\n",
    "\n",
    "            neighbor_ddf = dd.from_pandas(neighbor_pdf, npartitions=1)\n",
    "\n",
    "            # Prepare neighbor pickups\n",
    "            df_neighbors = df[['PULocationID', 'bin', 'pickups']].rename(\n",
    "                columns={'PULocationID': 'neighbor_id', 'pickups': 'neighbor_pickups'}\n",
    "            )\n",
    "\n",
    "            merged = neighbor_ddf.merge(df_neighbors, on='neighbor_id', how='left')\n",
    "\n",
    "            neighbor_demand_df = (\n",
    "                merged.groupby(['PULocationID', 'bin'])['neighbor_pickups']\n",
    "                .sum()\n",
    "                .reset_index()\n",
    "                .rename(columns={'neighbor_pickups': 'neighbor_pickups_sum'})\n",
    "            )\n",
    "\n",
    "            neighbor_demand_df['neighbor_pickups_sum'] = neighbor_demand_df['neighbor_pickups_sum']#.fillna(-1)\n",
    "\n",
    "            df = df.merge(neighbor_demand_df, on=['PULocationID', 'bin'], how='left')\n",
    "            df['neighbor_pickups_sum'] = df['neighbor_pickups_sum'].fillna(0)\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Unable to generate neighbor features\", exc_info=True)\n",
    "            raise RideDemandException(e, sys)\n",
    "        \n",
    "    \n",
    "    def engineer_autoregressive_signals(self, df: dd.DataFrame) -> dd.DataFrame:\n",
    "        try:\n",
    "            ## Define a Pandas function to apply per-partition\n",
    "            pdf= df[['PULocationID', 'bin', 'pickups', \n",
    "                     'city_pickups', 'neighbor_pickups_sum']].compute()\n",
    "\n",
    "            def make_lags(group, col='pickups'):\n",
    "\n",
    "                ## for lag features\n",
    "                for l in [1,24]:\n",
    "                    group[f'{col}_lag_{l}h'] = group[col].shift(l)\n",
    "\n",
    "                ## for rolling mean and std for zonelevel/bin\n",
    "                for w in [24]:\n",
    "                    group[f'{col}_roll_mean_{w}h'] = group[col].shift(1).rolling(w).mean()\n",
    "                    group[f'{col}_roll_std_{w}h'] = group[col].shift(1).rolling(w).std()\n",
    "                return group\n",
    "            \n",
    "            pdf.reset_index()\n",
    "            pdf = pdf.sort_values(['PULocationID','bin'])\n",
    "\n",
    "            ##generating the autoregressive feature\n",
    "            pdf = pdf.groupby('PULocationID', \n",
    "                              group_keys=False).apply(make_lags) \n",
    "\n",
    "            # Create lag features for city pickups(1h, 24h)\n",
    "            for lag in [1, 24]:\n",
    "                pdf[f'city_pickups_lag_{lag}h'] = pdf['city_pickups'].shift(lag)\n",
    "\n",
    "            ## computing the Lagged neighbor demand\n",
    "            for lag in [1,24]:\n",
    "                pdf[f'neighbor_pickups_lag_{lag}h'] = pdf.groupby(\n",
    "                    'PULocationID')['neighbor_pickups_sum'].shift(lag)\n",
    "\n",
    "            pdf.fillna(0, inplace=True)\n",
    "\n",
    "            df = df.merge(dd.from_pandas(pdf, npartitions=4), on=['PULocationID', \"bin\"], how='left')\n",
    "            df= df.rename(columns={'pickups_x':'pickups', 'city_pickups_x':'city_pickups', \n",
    "                       'neighbor_pickups_sum_x':'neighbor_pickups_sum'}\n",
    "                       )\n",
    "            \n",
    "            df = df.drop(['pickups_y', 'city_pickups_y', 'neighbor_pickups_sum_y'], axis=1)\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Failed to generate autoregressive features\", exc_info=True)\n",
    "            raise RideDemandException(e, sys)\n",
    "\n",
    "        \n",
    "    def save_data_to_feature_store(self, df):\n",
    "        try:\n",
    "            transformed_data_store = self.config.transformed_data_file_path\n",
    "\n",
    "            logger.info(\"Saving the transformed dataset to the feature store\")\n",
    "            os.makedirs(os.path.dirname(transformed_data_store), exist_ok=True)\n",
    "            df.to_parquet(transformed_data_store)\n",
    "\n",
    "            logger.info(f\"Transformed data saved to path: {transformed_data_store}\")\n",
    "            print(f\"Size of transformed data: {len(df)},{df.shape[1]}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Unable to save the file\", exc_info=True)\n",
    "            raise RideDemandException(e, sys)\n",
    "        \n",
    "\n",
    "    def push_transformed_data_to_feature_store(self, data)-> None:\n",
    "        try:\n",
    "            api = os.getenv('HOPSWORKS_API_KEY')\n",
    "            \n",
    "            ##initializing and login to hopswork feature store\n",
    "            project = hopsworks.login(project='RideDemandPrediction', api_key_value=api)\n",
    "            fs = project.get_feature_store()\n",
    "\n",
    "            ## creating a new feature group\n",
    "            fg = fs.get_or_create_feature_group(\n",
    "                name = 'ridedemandprediction',\n",
    "                version = 1,\n",
    "                primary_key = ['PULocationID', 'bin_str'],\n",
    "                event_time = 'bin',\n",
    "                description = 'NYC yellow taxi pickup demands per hour per zone',\n",
    "                online_enabled = False,\n",
    "                partition_key = ['pickup_year','pickup_month']\n",
    "            )\n",
    "\n",
    "            ##converting dask dataframe to pandas dataframe\n",
    "            data = data.compute()\n",
    "            \n",
    "            ## inserting new data in the feature group created above\n",
    "            fg.insert(data, storage = 'offline', write_options = {'wait_for_job': True, 'use_spark':True})\n",
    "\n",
    "            logger.info('data successfully added to hopsworks feature group')\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RideDemandException(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e8feffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.DynamicPricingEngine.config.configuration import ConfigurationManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d9010da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-09 20:02:59,986 INFO: successfully load the yaml file from path: config/config.yaml\n",
      "2026-01-09 20:02:59,997 INFO: successfully load the yaml file from path: params.yaml\n",
      "2026-01-09 20:02:59,998 INFO: successfully created directory at: ['artifacts']\n",
      "2026-01-09 20:02:59,999 INFO: Artifacts root directory successfully created: artifacts\n",
      "2026-01-09 20:03:00,000 INFO: successfully created directory at: ['artifacts/data_transformation']\n"
     ]
    }
   ],
   "source": [
    "taxi= 'artifacts/data_ingestion/taxi_data.parquet'\n",
    "weather = 'artifacts/data_ingestion/weather_data.csv'\n",
    "\n",
    "config = ConfigurationManager()\n",
    "transform_config = config.get_data_transformation_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e049f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map function that you are using.\n",
      "  Before: .map(func)\n",
      "  After:  .map(func, meta=('pickup_month', 'object'))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-09 20:06:50,823 INFO: Loaded neighbor dictionary from cache\n",
      "2026-01-09 20:07:01,970 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2026-01-09 20:07:01,973 INFO: Initializing external client\n",
      "2026-01-09 20:07:01,974 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2026-01-09 20:07:03,063 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1257642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Specifying the storage option is not supported if the streaming APIs are enabled\n"
     ]
    },
    {
     "ename": "RideDemandException",
     "evalue": "There is an error in /tmp/ipykernel_27338/2067020572.py at line 521 with The truth value of a DataFrame is ambiguous. Use a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 521\u001b[39m, in \u001b[36mDataTransformation.push_transformed_data_to_feature_store\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    520\u001b[39m \u001b[38;5;66;03m## inserting new data in the feature group created above\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m \u001b[43mfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moffline\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwait_for_job\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muse_spark\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    523\u001b[39m logger.info(\u001b[33m'\u001b[39m\u001b[33mdata successfully added to hopsworks feature group\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/hsfs/feature_group.py:2996\u001b[39m, in \u001b[36mFeatureGroup.insert\u001b[39m\u001b[34m(self, features, overwrite, operation, storage, write_options, validation_options, wait, transformation_context, transform)\u001b[39m\n\u001b[32m   2991\u001b[39m     warnings.warn(\n\u001b[32m   2992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSpecifying the storage option is not supported if the streaming APIs are enabled\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2993\u001b[39m         stacklevel=\u001b[32m1\u001b[39m,\n\u001b[32m   2994\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2996\u001b[39m feature_dataframe = \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_default_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2998\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m validation_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/hsfs/engine/python.py:750\u001b[39m, in \u001b[36mEngine.convert_to_default_dataframe\u001b[39m\u001b[34m(self, dataframe)\u001b[39m\n\u001b[32m    749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dataframe_copy\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m dataframe == \u001b[33m\"\u001b[39m\u001b[33mspine\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:454\u001b[39m, in \u001b[36mFrameBase.__bool__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__bool__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    455\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe truth value of a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is ambiguous. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    456\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUse a.any() or a.all().\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    457\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: The truth value of a DataFrame is ambiguous. Use a.any() or a.all().",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRideDemandException\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m df = transform.engineer_autoregressive_signals(df)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m#transform.save_data_to_feature_store(df)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpush_transformed_data_to_feature_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 526\u001b[39m, in \u001b[36mDataTransformation.push_transformed_data_to_feature_store\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    523\u001b[39m     logger.info(\u001b[33m'\u001b[39m\u001b[33mdata successfully added to hopsworks feature group\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RideDemandException(e,sys)\n",
      "\u001b[31mRideDemandException\u001b[39m: There is an error in /tmp/ipykernel_27338/2067020572.py at line 521 with The truth value of a DataFrame is ambiguous. Use a.any() or a.all()."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "transform = DataTransformation(transform_config, taxi, weather)\n",
    "df = transform.derive_target_and_join_to_weather_feature()\n",
    "df = transform.engineer_temporal_feature(df)\n",
    "df = transform.city_wide_congestion_features(df)\n",
    "df = transform.zone_level_congestion_features(df)\n",
    "df = transform.citywide_hourly_demand(df)\n",
    "df = transform.generate_neighbor_features(df)\n",
    "df = transform.engineer_autoregressive_signals(df)\n",
    "#transform.save_data_to_feature_store(df)\n",
    "transform.push_transformed_data_to_feature_store(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ec7cc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-09 20:01:43,881 INFO: Initializing external client\n",
      "2026-01-09 20:01:43,884 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2026-01-09 20:01:46,785 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1257642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Specifying the storage option is not supported if the streaming APIs are enabled\n"
     ]
    },
    {
     "ename": "RideDemandException",
     "evalue": "There is an error in /tmp/ipykernel_27338/2152897444.py at line 521 with The truth value of a DataFrame is ambiguous. Use a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 521\u001b[39m, in \u001b[36mDataTransformation.push_transformed_data_to_feature_store\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    520\u001b[39m \u001b[38;5;66;03m## inserting new data in the feature group created above\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m \u001b[43mfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moffline\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwait_for_job\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muse_spark\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    523\u001b[39m logger.info(\u001b[33m'\u001b[39m\u001b[33mdata successfully added to hopsworks feature group\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/hsfs/feature_group.py:2996\u001b[39m, in \u001b[36mFeatureGroup.insert\u001b[39m\u001b[34m(self, features, overwrite, operation, storage, write_options, validation_options, wait, transformation_context, transform)\u001b[39m\n\u001b[32m   2991\u001b[39m     warnings.warn(\n\u001b[32m   2992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSpecifying the storage option is not supported if the streaming APIs are enabled\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2993\u001b[39m         stacklevel=\u001b[32m1\u001b[39m,\n\u001b[32m   2994\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2996\u001b[39m feature_dataframe = \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_default_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2998\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m validation_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/hsfs/engine/python.py:750\u001b[39m, in \u001b[36mEngine.convert_to_default_dataframe\u001b[39m\u001b[34m(self, dataframe)\u001b[39m\n\u001b[32m    749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dataframe_copy\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m dataframe == \u001b[33m\"\u001b[39m\u001b[33mspine\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:454\u001b[39m, in \u001b[36mFrameBase.__bool__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__bool__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    455\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe truth value of a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is ambiguous. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    456\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUse a.any() or a.all().\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    457\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: The truth value of a DataFrame is ambiguous. Use a.any() or a.all().",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRideDemandException\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpush_transformed_data_to_feature_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 526\u001b[39m, in \u001b[36mDataTransformation.push_transformed_data_to_feature_store\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    523\u001b[39m     logger.info(\u001b[33m'\u001b[39m\u001b[33mdata successfully added to hopsworks feature group\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RideDemandException(e,sys)\n",
      "\u001b[31mRideDemandException\u001b[39m: There is an error in /tmp/ipykernel_27338/2152897444.py at line 521 with The truth value of a DataFrame is ambiguous. Use a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "transform.push_transformed_data_to_feature_store(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19fee3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b69e52ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>bin</th>\n",
       "      <th>pickups</th>\n",
       "      <th>datetime</th>\n",
       "      <th>temp</th>\n",
       "      <th>dew</th>\n",
       "      <th>humidity</th>\n",
       "      <th>precip</th>\n",
       "      <th>snow</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>...</th>\n",
       "      <th>neighbor_pickups_sum_x</th>\n",
       "      <th>pickups_lag_1h</th>\n",
       "      <th>pickups_lag_24h</th>\n",
       "      <th>pickups_roll_mean_24h</th>\n",
       "      <th>pickups_roll_std_24h</th>\n",
       "      <th>city_pickups_lag_1h</th>\n",
       "      <th>city_pickups_lag_24h</th>\n",
       "      <th>neighbor_pickups_lag_1h</th>\n",
       "      <th>neighbor_pickups_lag_24h</th>\n",
       "      <th>neighbor_pickups_sum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__null_dask_index__</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201</td>\n",
       "      <td>2025-11-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>53.5</td>\n",
       "      <td>28.6</td>\n",
       "      <td>38.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3204.0</td>\n",
       "      <td>5066.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201</td>\n",
       "      <td>2025-11-01 02:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>02:00:00</td>\n",
       "      <td>52.5</td>\n",
       "      <td>27.1</td>\n",
       "      <td>37.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9349.0</td>\n",
       "      <td>2724.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201</td>\n",
       "      <td>2025-11-01 05:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>05:00:00</td>\n",
       "      <td>50.4</td>\n",
       "      <td>27.1</td>\n",
       "      <td>40.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5217.0</td>\n",
       "      <td>976.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201</td>\n",
       "      <td>2025-11-01 15:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>15:00:00</td>\n",
       "      <td>57.2</td>\n",
       "      <td>29.5</td>\n",
       "      <td>34.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8744.0</td>\n",
       "      <td>6907.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201</td>\n",
       "      <td>2025-11-01 18:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>18:00:00</td>\n",
       "      <td>56.3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>35.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9825.0</td>\n",
       "      <td>6589.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     PULocationID                 bin  pickups  datetime  \\\n",
       "__null_dask_index__                                                        \n",
       "0                             201 2025-11-01 00:00:00        1  00:00:00   \n",
       "1                             201 2025-11-01 02:00:00        0  02:00:00   \n",
       "2                             201 2025-11-01 05:00:00        0  05:00:00   \n",
       "3                             201 2025-11-01 15:00:00        0  15:00:00   \n",
       "4                             201 2025-11-01 18:00:00        1  18:00:00   \n",
       "\n",
       "                     temp   dew  humidity  precip  snow  windspeed  ...  \\\n",
       "__null_dask_index__                                                 ...   \n",
       "0                    53.5  28.6     38.14     0.0   0.0       13.6  ...   \n",
       "1                    52.5  27.1     37.29     0.0   0.0       16.4  ...   \n",
       "2                    50.4  27.1     40.29     0.0   0.0       12.1  ...   \n",
       "3                    57.2  29.5     34.57     0.0   0.0       14.0  ...   \n",
       "4                    56.3  29.0     35.08     0.0   0.0        9.2  ...   \n",
       "\n",
       "                     neighbor_pickups_sum_x  pickups_lag_1h  pickups_lag_24h  \\\n",
       "__null_dask_index__                                                            \n",
       "0                                         0             0.0              0.0   \n",
       "1                                         0             0.0              0.0   \n",
       "2                                         0             0.0              0.0   \n",
       "3                                         0             1.0              0.0   \n",
       "4                                         0             0.0              0.0   \n",
       "\n",
       "                     pickups_roll_mean_24h  pickups_roll_std_24h  \\\n",
       "__null_dask_index__                                                \n",
       "0                                      0.0                   0.0   \n",
       "1                                      0.0                   0.0   \n",
       "2                                      0.0                   0.0   \n",
       "3                                      0.0                   0.0   \n",
       "4                                      0.0                   0.0   \n",
       "\n",
       "                     city_pickups_lag_1h  city_pickups_lag_24h  \\\n",
       "__null_dask_index__                                              \n",
       "0                                 3204.0                5066.0   \n",
       "1                                 9349.0                2724.0   \n",
       "2                                 5217.0                 976.0   \n",
       "3                                 8744.0                6907.0   \n",
       "4                                 9825.0                6589.0   \n",
       "\n",
       "                     neighbor_pickups_lag_1h neighbor_pickups_lag_24h  \\\n",
       "__null_dask_index__                                                     \n",
       "0                                        0.0                      0.0   \n",
       "1                                        0.0                      0.0   \n",
       "2                                        0.0                      0.0   \n",
       "3                                        0.0                      0.0   \n",
       "4                                        0.0                      0.0   \n",
       "\n",
       "                     neighbor_pickups_sum  \n",
       "__null_dask_index__                        \n",
       "0                                       0  \n",
       "1                                       0  \n",
       "2                                       0  \n",
       "3                                       0  \n",
       "4                                       0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_parquet('artifacts/data_transformation/transformed_data.parquet')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ed6d5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PULocationID', 'bin', 'pickups', 'datetime', 'temp', 'dew', 'humidity',\n",
       "       'precip', 'snow', 'windspeed', 'feelslike', 'snowdepth', 'visibility',\n",
       "       'pickup_year', 'pickup_month', 'day_of_month', 'Pickup_hour',\n",
       "       'day_of_week', 'bin_str', 'is_weekend', 'is_rush_hour', 'is_night_hour',\n",
       "       'season_of_year', 'is_holiday', 'Is_special_event', 'is_payday',\n",
       "       'city_avg_speed', 'city_congestion_index', 'zone_avg_speed',\n",
       "       'zone_congestion_index', 'city_pickups', 'neighbor_pickups_sum_x',\n",
       "       'pickups_lag_1h', 'pickups_lag_24h', 'pickups_roll_mean_24h',\n",
       "       'pickups_roll_std_24h', 'city_pickups_lag_1h', 'city_pickups_lag_24h',\n",
       "       'neighbor_pickups_lag_1h', 'neighbor_pickups_lag_24h',\n",
       "       'neighbor_pickups_sum'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "693b9c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int8')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Is_special_event'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42bd05e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data['bin'] = pd.to_datetime(data['bin'])\n",
    "data['pickups'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int8')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['is_holiday'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'neighbor_pickups_sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py:3791\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3790\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3791\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3792\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:152\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:181\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'neighbor_pickups_sum'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mneighbor_pickups_sum\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/frame.py:3893\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3891\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   3892\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m3893\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3894\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   3895\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py:3798\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3793\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3794\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3795\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3796\u001b[39m     ):\n\u001b[32m   3797\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3798\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3799\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3800\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3801\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3802\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3803\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'neighbor_pickups_sum'"
     ]
    }
   ],
   "source": [
    "data['neighbor_pickups_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PULocationID                0\n",
       "bin                         0\n",
       "pickups                     0\n",
       "datetime                    0\n",
       "temp                        0\n",
       "dew                         0\n",
       "humidity                    0\n",
       "precip                      0\n",
       "snow                        0\n",
       "windspeed                   0\n",
       "feelslike                   0\n",
       "snowdepth                   0\n",
       "visibility                  0\n",
       "pickup_year                 0\n",
       "pickup_month                0\n",
       "day_of_month                0\n",
       "Pickup_hour                 0\n",
       "day_of_week                 0\n",
       "bin_str                     0\n",
       "is_weekend                  0\n",
       "is_rush_hour                0\n",
       "is_night_hour               0\n",
       "season_of_year              0\n",
       "is_holiday                  0\n",
       "Is_special_event            0\n",
       "is_payday                   0\n",
       "city_avg_speed              0\n",
       "city_congestion_index       0\n",
       "zone_avg_speed              0\n",
       "zone_congestion_index       0\n",
       "city_pickups                0\n",
       "neighbor_pickups_sum_x      0\n",
       "pickups_lag_1h              0\n",
       "pickups_lag_24h             0\n",
       "pickups_roll_mean_24h       0\n",
       "pickups_roll_std_24h        0\n",
       "city_pickups_lag_1h         0\n",
       "city_pickups_lag_24h        0\n",
       "neighbor_pickups_lag_1h     0\n",
       "neighbor_pickups_lag_24h    0\n",
       "neighbor_pickups_sum        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "225afaf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(191100, 41)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7113473f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
