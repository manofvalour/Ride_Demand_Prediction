{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90ee5e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/emmanuel/Desktop/Dynamic-Pricing-Engine'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5ba659b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "313cf088",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbg = XGBRegressor(n_job=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0321036",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2942635986.py, line 4)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdatetime.today().strftime('%Y-%m-%d').\u001b[39m\n                                          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import timedelta, datetime\n",
    "#extracting the date\n",
    "datetime.today().strftime('%Y-%m-%d')\n",
    "#datetime.today()# - relativedelta(months=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f58e64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.DynamicPricingEngine.utils.data_ingestion_utils import time_subtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a20a3d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start date: 2025-10-01, end date: 2025-10-31\n"
     ]
    }
   ],
   "source": [
    "## two months into the past\n",
    "now = (datetime.today()- relativedelta(months=1))\n",
    "# end_date = datetime.strptime(now, \"%Y-%m-%d\") ## converting to datetime\n",
    "end_date = now - timedelta(days=now.day) ## retrieving the last day of the previous month\n",
    "\n",
    "## accessing the previous month\n",
    "days_to_subtract = time_subtract(end_date.strftime('%Y-%m-%d'))\n",
    "end_date = (end_date - timedelta(days=days_to_subtract))\n",
    "\n",
    "## start of the month\n",
    "days= time_subtract(end_date.strftime('%Y-%m-%d'))\n",
    "start_date= end_date - timedelta(days=days-1)\n",
    "\n",
    "#self.config = config\n",
    "start_date = start_date.strftime('%Y-%m-%d')\n",
    "end_date = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "print(f'start date: {start_date}, end date: {end_date}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b78063",
   "metadata": {},
   "source": [
    "## Exploratory Data Anaysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30685d8",
   "metadata": {},
   "source": [
    "### Things to do:\n",
    "- Data Collection\n",
    "- Data Checks\n",
    "- Exploratory Data Analysis\n",
    "- Data Preproccessing and Feature Engineering\n",
    "- Model Training, Evaluation, and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4473780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2026-01-17 14:52:16,436 ]: matplotlib.font_manager: INFO: font_manager: 1107: Failed to extract font properties from /usr/share/fonts/noto/NotoColorEmoji.ttf: Can not load face (unknown file format; error code 0x2)\n",
      "[ 2026-01-17 14:52:16,542 ]: matplotlib.font_manager: INFO: font_manager: 1639: generated new fontManager\n"
     ]
    }
   ],
   "source": [
    "## importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_html = pd.read_html('https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)',match = 'by country')\n",
    "# Let's see how many tables are there with tage ' by county'\n",
    "print(len(df_html)) # There are 4 tables\n",
    "# Let's see the first table\n",
    "df_html[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa5b70b",
   "metadata": {},
   "source": [
    "### Extracting Weather Information for NYC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b7fa4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73f517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql import SparkSession\n",
    "\n",
    "spark = ('dynamicpricing').getOrCreate()\n",
    "#spark = (\n",
    " #   SparkSession.builder\n",
    "  #      .master(\"local\")\n",
    "   #     .appName(\"dynamicPricing\")\n",
    "    #    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a7afe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['JAVA_HOME']= "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55129de",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SPARK_HOME']= 'opt/apache-spark/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22da3ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f85c6a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emmanuel/Desktop/Dynamic-Pricing-Engine/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys, pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import dask.dataframe as dd\n",
    "from datetime import datetime, timedelta\n",
    "import hopsworks\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from src.DynamicPricingEngine.logger.logger import logger\n",
    "from src.DynamicPricingEngine.exception.customexception import RideDemandException\n",
    "from src.DynamicPricingEngine.entity.config_entity import DataTransformationConfig\n",
    "from src.DynamicPricingEngine.utils.common_utils import load_shapefile_from_zip\n",
    "from src.DynamicPricingEngine.utils.data_ingestion_utils import time_subtract\n",
    "\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig, \n",
    "                 nyc_taxi_data: str, \n",
    "                 nyc_weather_data: str\n",
    "                 ):\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        #Read both datasets with Dask\n",
    "        self.taxi_df  =dd.read_parquet(nyc_taxi_data, engine='pyarrow')\n",
    "        self.weather_df =dd.read_csv(nyc_weather_data)\n",
    "\n",
    "        self.taxi_df.index = self.taxi_df.index.astype('int32')\n",
    "        self.weather_df.index = self.weather_df.index.astype('int32')\n",
    "\n",
    "        #Ensure datetime types\n",
    "        for col in ['tpep_pickup_datetime', 'tpep_dropoff_datetime']:\n",
    "            if col in self.taxi_df.columns:\n",
    "                self.taxi_df[col] = dd.to_datetime(self.taxi_df[col], errors='coerce')\n",
    "\n",
    "        # Precompute bin\n",
    "        if 'tpep_pickup_datetime' in self.taxi_df.columns:\n",
    "            self.taxi_df['bin'] = self.taxi_df['tpep_pickup_datetime'].dt.floor('60min')\n",
    "\n",
    "         #Cache neighbor dictionary\n",
    "        self._neighbor_dict = None\n",
    "        self._neighbor_cache_path = os.path.join(self.config.shapefile_dir, \"neighbors.pkl\")\n",
    "\n",
    "    def _get_neighbor_dict(self) -> dict:\n",
    "        if self._neighbor_dict is not None:\n",
    "            return self._neighbor_dict\n",
    "\n",
    "        if os.path.exists(self._neighbor_cache_path):\n",
    "            try:\n",
    "                with open(self._neighbor_cache_path, \"rb\") as f:\n",
    "                    self._neighbor_dict = pickle.load(f)\n",
    "                logger.info(\"Loaded neighbor dictionary from cache\")\n",
    "                return self._neighbor_dict\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to load neighbor cache: {e}\")\n",
    "\n",
    "        zones_gdf = load_shapefile_from_zip(self.config.taxi_zone_shapefile_url, \n",
    "                                            self.config.shapefile_dir)\n",
    "        zones_gdf_left = zones_gdf.rename(columns={\"LocationID\": \"LocationID_left\"})\n",
    "        zones_gdf_right = zones_gdf.rename(columns={\"LocationID\": \"LocationID_right\"})\n",
    "        neighbors_df = gpd.sjoin(zones_gdf_left, zones_gdf_right, how=\"left\", predicate=\"touches\")\n",
    "        neighbors_df = neighbors_df[neighbors_df['LocationID_left'] != neighbors_df['LocationID_right']]\n",
    "        self._neighbor_dict = (neighbors_df.groupby('LocationID_left')['LocationID_right']\n",
    "                               .apply(lambda s: sorted(list(set(s))))\n",
    "                               .to_dict())\n",
    "        try:\n",
    "            os.makedirs(self.config.shapefile_dir, exist_ok=True)\n",
    "            with open(self._neighbor_cache_path, \"wb\") as f:\n",
    "                pickle.dump(self._neighbor_dict, f)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to persist neighbor cache: {e}\")\n",
    "        return self._neighbor_dict\n",
    "\n",
    "\n",
    "    def derive_target_and_join_to_weather_feature(self) -> dd.DataFrame:\n",
    "        try:\n",
    "            taxi_df = self.taxi_df[['PULocationID', 'bin']]\n",
    "\n",
    "            # Aggregate pickups per zone-hour\n",
    "            y = (taxi_df\n",
    "                 .groupby(['PULocationID', 'bin'])\n",
    "                 .size().rename('pickups')\n",
    "                 .reset_index())\n",
    "            \n",
    "            y['bin']= y['bin'].astype('datetime64[ns]')\n",
    "            y['PULocationID']= y['PULocationID'].astype('int32')\n",
    "\n",
    "            # Materialized data to Pandas to build full grid then back to Dask\n",
    "            zones = y['PULocationID'].unique().compute()\n",
    "\n",
    "            time_index = pd.date_range(y['bin'].min().compute(), \n",
    "                                       y['bin'].max().compute(), \n",
    "                                       freq='60min')\n",
    "            \n",
    "            grid = pd.MultiIndex.from_product([zones, time_index], \n",
    "                                              names=['PULocationID', \n",
    "                                                     'bin']).to_frame(index=False)\n",
    "\n",
    "            #y['PULocationID'] = y['PULocationID'].astype('int32')\n",
    "            grid['PULocationID'] = grid['PULocationID'].astype('int32')\n",
    "\n",
    "            # Align datetime precision\n",
    "            y['bin'] = y['bin'].astype('datetime64[ns]')\n",
    "            grid['bin'] = grid['bin'].astype('datetime64[ns]')\n",
    "\n",
    "            y = dd.from_pandas(grid, npartitions=4).merge(y, how='left', \n",
    "                                                          on=['PULocationID', 'bin'])\n",
    "            y = y.fillna({'pickups': 0})\n",
    "\n",
    "            # Weather alignment\n",
    "            weather_df = self.weather_df\n",
    "            weather_df['bin'] = dd.to_datetime(\n",
    "                weather_df['day'].astype(str) + ' ' + \n",
    "                weather_df['datetime'].astype(str),\n",
    "                errors='coerce'\n",
    "            )\n",
    "            \n",
    "            #Dropping the Day column\n",
    "            weather_df = weather_df.drop(columns='day')\n",
    "            y.index = y.index.astype('int32')\n",
    "\n",
    "            # Merge target + weather and sort by PUlocationID and bin\n",
    "            df = y.merge(weather_df, on='bin', how='left').map_partitions(\n",
    "                lambda pdf: pdf.sort_values(['PULocationID', 'bin'])\n",
    "            )\n",
    "\n",
    "            return (df)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Failed to generate the target feature\", exc_info=True)\n",
    "            raise RideDemandException(e, sys)\n",
    "        \n",
    "    def engineer_temporal_feature(self, df: dd.DataFrame) -> dd.DataFrame:\n",
    "        \"\"\"\n",
    "        Deriving the temporal feature\n",
    "        \n",
    "        :type df: dd.DataFrame\n",
    "        :return: Description\n",
    "        :rtype: DataFrame\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Temporal from 'bin'\n",
    "            df['pickup_year'] = df['bin'].dt.year\n",
    "            df['pickup_month'] = df['bin'].dt.month\n",
    "            df['day_of_month'] = df['bin'].dt.day\n",
    "            df['Pickup_hour'] = df['bin'].dt.hour\n",
    "            df['day_of_week'] = df['bin'].dt.dayofweek\n",
    "            df[\"bin_str\"] = df[\"bin\"].astype('str')\n",
    "\n",
    "            # Vectorized flags\n",
    "            df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype('int8')\n",
    "            df['is_rush_hour'] = df['Pickup_hour'].isin([7, 8, 9, 16, 17, 18, 19]).astype('int8')\n",
    "            df['is_night_hour'] = df['Pickup_hour'].isin([0,1,2,3,4,5,6,20,21,22,23]).astype('int8')\n",
    "\n",
    "            # Season mapping\n",
    "            season_map = {\n",
    "                12: 'winter', 1: 'winter', 2: 'winter',\n",
    "                3: 'spring', 4: 'spring', 5: 'spring',\n",
    "                6: 'summer', 7: 'summer', 8: 'summer',\n",
    "                9: 'autumn', 10: 'autumn', 11: 'autumn'\n",
    "            }\n",
    "            df['season_of_year'] = df['pickup_month'].map(season_map)\n",
    "\n",
    "            # Fixed holidays and specials\n",
    "            fixed_holidays = {(1, 1), (7, 4), (11, 11), (6, 19), (12, 25)}\n",
    "            fixed_specials = {(3, 17), (7, 4), (6, 4), (12, 31),\n",
    "                            (6, 5), (6, 6), (6, 7), (6, 8), (6, 9), (6, 10),\n",
    "                            (6, 11), (6, 12), (6, 13), (6, 14), (6, 15)}\n",
    "\n",
    "            df['is_holiday'] = df[['pickup_month', 'day_of_month']].map_partitions(\n",
    "                lambda pdf: pdf.apply(lambda r: int((r['pickup_month'], \n",
    "                                                     r['day_of_month']) in fixed_holidays), axis=1),\n",
    "                meta=('is_holiday', 'int8')\n",
    "            )\n",
    "\n",
    "            df['Is_special_event'] = df[['pickup_month', 'day_of_month']].map_partitions(\n",
    "                lambda pdf: pdf.apply(lambda r: int((r['pickup_month'], \n",
    "                                                     r['day_of_month']) in fixed_specials), axis=1),\n",
    "                meta=('Is_special_event', 'int8')\n",
    "            )\n",
    "\n",
    "            # Movable holidays and specials (row-wise logic via map_partitions)\n",
    "            def add_movable_flags(pdf: dd.DataFrame) -> dd.DataFrame:\n",
    "                def _movable_holiday(row):\n",
    "                    y, m, d = row['pickup_year'], row['pickup_month'], row['day_of_month']\n",
    "                    date = pd.Timestamp(y, m, d)\n",
    "                    if m == 11 and date.weekday() == 3 and date + pd.Timedelta(days=7) > pd.Timestamp(y, 11, 30):\n",
    "                        return 1\n",
    "                    if m == 5 and date.weekday() == 0 and date + pd.Timedelta(days=7) > pd.Timestamp(y, 5, 31):\n",
    "                        return 1\n",
    "                    if m == 1 and date.weekday() == 0 and 15 <= d <= 21:\n",
    "                        return 1\n",
    "                    if m == 9 and date.weekday() == 0 and 1 <= d <= 7:\n",
    "                        return 1\n",
    "                    if m == 11 and date.weekday() == 1 and 1 <= d <= 7:\n",
    "                        return 1\n",
    "                    if m == 10 and date.weekday() == 0 and 8 <= d <= 14:\n",
    "                        return 1\n",
    "                    return 0\n",
    "\n",
    "                def _movable_special(row):\n",
    "                    y, m, d = row['pickup_year'], row['pickup_month'], row['day_of_month']\n",
    "                    date = pd.Timestamp(y, m, d)\n",
    "                    if m == 11 and date.weekday() == 3 and date + pd.Timedelta(days=7) > pd.Timestamp(y, 11, 30):\n",
    "                        return 1\n",
    "                    if m == 6 and date.weekday() == 6 and date + pd.Timedelta(days=7) > pd.Timestamp(y, 6, 30):\n",
    "                        return 1\n",
    "                    return 0\n",
    "\n",
    "                pdf['is_holiday'] = pdf['is_holiday'].where(pdf['is_holiday'] == 1,\n",
    "                                                        pdf.apply(_movable_holiday, axis=1))\n",
    "                pdf.loc[pdf['Is_special_event'] == 0, 'Is_special_event'] = pdf.loc[pdf['Is_special_event'] == 0].apply(_movable_special, axis=1)\n",
    "                \n",
    "                return pdf\n",
    "\n",
    "            df = df.map_partitions(add_movable_flags) #meta=df._meta)\n",
    "\n",
    "            ## creating a column for Payday Indicator\n",
    "            def is_payday(data):\n",
    "\n",
    "                date = pd.Timestamp(year=data['pickup_year'], \n",
    "                                    month=data['pickup_month'], \n",
    "                                    day=data['day_of_month'])\n",
    "\n",
    "                if date.is_month_end:\n",
    "                    return 1\n",
    "                \n",
    "                if date.day ==(15 or 16 or 17) and date.isoweekday!=(6 or 7):\n",
    "                    return 1\n",
    "\n",
    "                return 0\n",
    "\n",
    "            #pdf = df.compute()\n",
    "\n",
    "            pdf= df[['PULocationID', 'bin', 'pickup_year', \n",
    "                     'pickup_month', 'day_of_month']].compute()\n",
    "\n",
    "            pdf['is_payday'] = pdf.apply(is_payday, axis=1) ##deriving the payday indicator\n",
    "\n",
    "            df = df.merge(dd.from_pandas(pdf, npartitions=4), \n",
    "                          on=['PULocationID', \"bin\"], how='left')\n",
    "            df= df.rename(columns={'pickup_year_x':'pickup_year', \n",
    "                                   'pickup_month_x':'pickup_month', \n",
    "                                   'day_of_month_x':'day_of_month'})\n",
    "            \n",
    "            df = df.drop(['pickup_year_y', 'pickup_month_y', \n",
    "                          'day_of_month_y'], axis=1)\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Failed to engineer temporal features\", exc_info=True)\n",
    "            raise RideDemandException(e, sys)\n",
    "\n",
    "\n",
    "    def city_wide_congestion_features(self, df: pd.DataFrame) -> dd.DataFrame:\n",
    "        try:\n",
    "            # Select needed columns\n",
    "            taxi_df = self.taxi_df[['bin', 'tpep_pickup_datetime', \n",
    "                                    'tpep_dropoff_datetime', 'trip_distance']]\n",
    "\n",
    "            # Compute trip duration in hours\n",
    "            taxi_df['trip_duration_hr'] = (\n",
    "                (taxi_df['tpep_dropoff_datetime'] - \n",
    "                 taxi_df['tpep_pickup_datetime']).dt.total_seconds() / 3600\n",
    "            )\n",
    "\n",
    "            # Filter invalid trips\n",
    "            taxi_df = taxi_df[(taxi_df['trip_duration_hr'] > 0) & \n",
    "                              (taxi_df['trip_distance'] >= 0)]\n",
    "\n",
    "            # Compute speed\n",
    "            taxi_df['MPH'] = taxi_df['trip_distance'] / taxi_df['trip_duration_hr']\n",
    "            taxi_df = taxi_df[taxi_df['MPH'].between(1, 60)]\n",
    "\n",
    "            # Compute citywide average speed per hour\n",
    "            city_speed = (\n",
    "                taxi_df.groupby('bin')['MPH']\n",
    "                .mean()\n",
    "                .reset_index()\n",
    "                .rename(columns={'MPH': 'city_avg_speed'})\n",
    "            )\n",
    "\n",
    "            # Congestion index\n",
    "            city_speed['city_congestion_index'] = city_speed['city_avg_speed'].map_partitions(\n",
    "                lambda pdf: np.where(pdf > 0, 1.0 / pdf, np.nan),\n",
    "                meta=('city_congestion_index', 'f8')\n",
    "            )\n",
    "\n",
    "            # Merge back into main df\n",
    "            df = df.merge(city_speed, on='bin', how='left')\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Unable to generate city-wide features\", exc_info=True)\n",
    "            raise RideDemandException(e, sys)\n",
    "\n",
    "\n",
    "    def zone_level_congestion_features(self, df: dd.DataFrame) -> dd.DataFrame:\n",
    "        try:\n",
    "            # Select needed columns\n",
    "            taxi_df = self.taxi_df[['PULocationID', 'bin', 'tpep_pickup_datetime',\n",
    "                                    'tpep_dropoff_datetime', 'trip_distance']]\n",
    "\n",
    "            # Compute trip duration in hours\n",
    "            taxi_df['trip_duration_hr'] = (\n",
    "                (taxi_df['tpep_dropoff_datetime'] \n",
    "                 -taxi_df['tpep_pickup_datetime'])\n",
    "                 .dt.total_seconds() / 3600\n",
    "            )\n",
    "\n",
    "            # Filter invalid trips\n",
    "            taxi_df = taxi_df[(taxi_df['trip_duration_hr'] > 0) & (taxi_df['trip_distance'] >= 0)]\n",
    "\n",
    "            # Compute speed\n",
    "            taxi_df['MPH'] = taxi_df['trip_distance'] / taxi_df['trip_duration_hr']\n",
    "            taxi_df = taxi_df[taxi_df['MPH'].between(1, 60)]\n",
    "\n",
    "            # Compute zone-level average speed per hour\n",
    "            zone_speed = (\n",
    "                taxi_df.groupby(['PULocationID', 'bin'])['MPH']\n",
    "                .mean()\n",
    "                .reset_index()\n",
    "                .rename(columns={'MPH': 'zone_avg_speed'})\n",
    "            )\n",
    "\n",
    "            # Congestion index\n",
    "            zone_speed['zone_congestion_index'] = zone_speed['zone_avg_speed'].map_partitions(\n",
    "                lambda pdf: np.where(pdf > 0, 1.0 / pdf, np.nan),\n",
    "                meta=('zone_congestion_index', 'f8')\n",
    "            )\n",
    "\n",
    "            # Build full grid (zones Ã— time) in Pandas, then convert back to Dask\n",
    "            zones = df['PULocationID'].unique().compute()\n",
    "            time_index = pd.date_range(df['bin'].min().compute(), \n",
    "                                       df['bin'].max().compute(), freq='60min'\n",
    "                                       )\n",
    "            \n",
    "            grid = pd.MultiIndex.from_product([zones, time_index], \n",
    "                                              names=['PULocationID', \n",
    "                                                     'bin']).to_frame(index=False\n",
    "                                                                      )\n",
    "            \n",
    "            grid_dd = dd.from_pandas(grid, npartitions=4)\n",
    "\n",
    "            zone_speed['PULocationID']= zone_speed['PULocationID'].astype('int32')\n",
    "\n",
    "            # Merge grid with zone_speed to ensure full coverage\n",
    "            zone_speed = grid_dd.merge(zone_speed, how='left', on=['PULocationID', 'bin'])\n",
    "            zone_speed[['zone_avg_speed', 'zone_congestion_index']] = zone_speed[\n",
    "                ['zone_avg_speed', 'zone_congestion_index']\n",
    "            ].fillna(0)\n",
    "\n",
    "            #zone_speed['PULocationID']= zone_speed['PULocationID'].astype('int32')\n",
    "            # Merge back into main df\n",
    "            df = df.merge(zone_speed, on=['PULocationID', 'bin'], how='left')\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Unable to generate zone-level features\", exc_info=True)\n",
    "            raise RideDemandException(e, sys)\n",
    "\n",
    "\n",
    "    def citywide_hourly_demand(self, df: dd.DataFrame) -> dd.DataFrame:\n",
    "        try:\n",
    "            # Aggregate citywide pickups per hour\n",
    "            city_demand = (\n",
    "                df.groupby('bin')['pickups']\n",
    "                .sum()\n",
    "                .reset_index()\n",
    "                .rename(columns={'pickups': 'city_pickups'})\n",
    "            )\n",
    "\n",
    "            # Merge back into main df\n",
    "            df = df.merge(city_demand, on='bin', how='left')\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Unable to engineer citywide hourly demand features\", exc_info=True)\n",
    "            raise RideDemandException(e, sys)\n",
    "\n",
    "\n",
    "    def generate_neighbor_features(self, df: dd.DataFrame) -> dd.DataFrame:\n",
    "        try:\n",
    "            neighbor_dict = self._get_neighbor_dict()\n",
    "\n",
    "            # Build neighbor pairs in Pandas, then convert to Dask\n",
    "            neighbor_pdf = pd.DataFrame(\n",
    "                [(zone, n) for zone, neighs in neighbor_dict.items() for n in neighs],\n",
    "                columns=['PULocationID', 'neighbor_id']\n",
    "            ).fillna(-1)\n",
    "\n",
    "            neighbor_pdf['PULocationID'] = neighbor_pdf['PULocationID'].astype(\n",
    "                df['PULocationID'].dtype\n",
    "                )\n",
    "            neighbor_pdf['neighbor_id'] = neighbor_pdf['neighbor_id'].astype(\n",
    "                df['PULocationID'].dtype\n",
    "                )\n",
    "\n",
    "            neighbor_ddf = dd.from_pandas(neighbor_pdf, npartitions=1)\n",
    "\n",
    "            # Prepare neighbor pickups\n",
    "            df_neighbors = df[['PULocationID', 'bin', 'pickups']].rename(\n",
    "                columns={'PULocationID': 'neighbor_id', 'pickups': 'neighbor_pickups'}\n",
    "            )\n",
    "\n",
    "            merged = neighbor_ddf.merge(df_neighbors, on='neighbor_id', how='left')\n",
    "\n",
    "            neighbor_demand_df = (\n",
    "                merged.groupby(['PULocationID', 'bin'])['neighbor_pickups']\n",
    "                .sum()\n",
    "                .reset_index()\n",
    "                .rename(columns={'neighbor_pickups': 'neighbor_pickups_sum'})\n",
    "            )\n",
    "\n",
    "            neighbor_demand_df['neighbor_pickups_sum'] = neighbor_demand_df['neighbor_pickups_sum']#.fillna(-1)\n",
    "\n",
    "            df = df.merge(neighbor_demand_df, on=['PULocationID', 'bin'], how='left')\n",
    "            df['neighbor_pickups_sum'] = df['neighbor_pickups_sum'].fillna(0)\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Unable to generate neighbor features\", exc_info=True)\n",
    "            raise RideDemandException(e, sys)\n",
    "        \n",
    "    \n",
    "    def engineer_autoregressive_signals(self, df: dd.DataFrame) -> dd.DataFrame:\n",
    "        try:\n",
    "            ## Define a Pandas function to apply per-partition\n",
    "            pdf= df[['PULocationID', 'bin', 'pickups', \n",
    "                     'city_pickups', 'neighbor_pickups_sum']].compute()\n",
    "\n",
    "            def make_lags(group, col='pickups'):\n",
    "\n",
    "                ## for lag features\n",
    "                for l in [1,24]:\n",
    "                    group[f'{col}_lag_{l}h'] = group[col].shift(l)\n",
    "\n",
    "                ## for rolling mean and std for zonelevel/bin\n",
    "                for w in [24]:\n",
    "                    group[f'{col}_roll_mean_{w}h'] = group[col].shift(1).rolling(w).mean()\n",
    "                    group[f'{col}_roll_std_{w}h'] = group[col].shift(1).rolling(w).std()\n",
    "                return group\n",
    "            \n",
    "            pdf.reset_index()\n",
    "            pdf = pdf.sort_values(['PULocationID','bin'])\n",
    "\n",
    "            ##generating the autoregressive feature\n",
    "            pdf = pdf.groupby('PULocationID', \n",
    "                              group_keys=False).apply(make_lags) \n",
    "\n",
    "            # Create lag features for city pickups(1h, 24h)\n",
    "            for lag in [1, 24]:\n",
    "                pdf[f'city_pickups_lag_{lag}h'] = pdf['city_pickups'].shift(lag)\n",
    "\n",
    "            ## computing the Lagged neighbor demand\n",
    "            for lag in [1,24]:\n",
    "                pdf[f'neighbor_pickups_lag_{lag}h'] = pdf.groupby(\n",
    "                    'PULocationID')['neighbor_pickups_sum'].shift(lag)\n",
    "\n",
    "            pdf.fillna(0, inplace=True)\n",
    "\n",
    "            df = df.merge(dd.from_pandas(pdf, npartitions=4), on=['PULocationID', \"bin\"], how='left')\n",
    "            df= df.rename(columns={'pickups_x':'pickups', 'city_pickups_x':'city_pickups', \n",
    "                       'neighbor_pickups_sum_x':'neighbor_pickups_sum'}\n",
    "                       )\n",
    "            \n",
    "            df = df.drop(['pickups_y', 'city_pickups_y', 'neighbor_pickups_sum_y'], axis=1)\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Failed to generate autoregressive features\", exc_info=True)\n",
    "            raise RideDemandException(e, sys)\n",
    "\n",
    "        \n",
    "    def save_data_to_feature_store(self, df):\n",
    "        try:\n",
    "            transformed_data_store = self.config.transformed_data_file_path\n",
    "\n",
    "            logger.info(\"Saving the transformed dataset to the feature store\")\n",
    "            os.makedirs(os.path.dirname(transformed_data_store), exist_ok=True)\n",
    "            df.to_parquet(transformed_data_store)\n",
    "\n",
    "            logger.info(f\"Transformed data saved to path: {transformed_data_store}\")\n",
    "            print(f\"Size of transformed data: {len(df)},{df.shape[1]}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Unable to save the file\", exc_info=True)\n",
    "            raise RideDemandException(e, sys)\n",
    "        \n",
    "\n",
    "    def push_transformed_data_to_feature_store(self, data)-> None:\n",
    "        try:\n",
    "            api = os.getenv('HOPSWORKS_API_KEY')\n",
    "            \n",
    "            ##initializing and login to hopswork feature store\n",
    "            project = hopsworks.login(project='RideDemandPrediction', api_key_value=api)\n",
    "            fs = project.get_feature_store()\n",
    "\n",
    "            ## creating a new feature group\n",
    "            fg = fs.get_or_create_feature_group(\n",
    "                name = 'ridedemandprediction',\n",
    "                version = 1,\n",
    "                primary_key = ['PULocationID', 'bin_str'],\n",
    "                event_time = 'bin',\n",
    "                description = 'NYC yellow taxi pickup demands per hour per zone',\n",
    "                online_enabled = False,\n",
    "                partition_key = ['pickup_year','pickup_month']\n",
    "            )\n",
    "\n",
    "            ##converting dask dataframe to pandas dataframe\n",
    "            data = data.compute()\n",
    "            \n",
    "            ## inserting new data in the feature group created above\n",
    "            fg.insert(data, storage = 'offline', write_options = {'wait_for_job': True, 'use_spark':True})\n",
    "\n",
    "            logger.info('data successfully added to hopsworks feature group')\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RideDemandException(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8feffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.DynamicPricingEngine.config.configuration import ConfigurationManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d9010da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-09 20:02:59,986 INFO: successfully load the yaml file from path: config/config.yaml\n",
      "2026-01-09 20:02:59,997 INFO: successfully load the yaml file from path: params.yaml\n",
      "2026-01-09 20:02:59,998 INFO: successfully created directory at: ['artifacts']\n",
      "2026-01-09 20:02:59,999 INFO: Artifacts root directory successfully created: artifacts\n",
      "2026-01-09 20:03:00,000 INFO: successfully created directory at: ['artifacts/data_transformation']\n"
     ]
    }
   ],
   "source": [
    "taxi= 'artifacts/data_ingestion/taxi_data.parquet'\n",
    "weather = 'artifacts/data_ingestion/weather_data.csv'\n",
    "\n",
    "config = ConfigurationManager()\n",
    "transform_config = config.get_data_transformation_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e049f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map function that you are using.\n",
      "  Before: .map(func)\n",
      "  After:  .map(func, meta=('pickup_month', 'object'))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-09 20:06:50,823 INFO: Loaded neighbor dictionary from cache\n",
      "2026-01-09 20:07:01,970 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2026-01-09 20:07:01,973 INFO: Initializing external client\n",
      "2026-01-09 20:07:01,974 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2026-01-09 20:07:03,063 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1257642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Specifying the storage option is not supported if the streaming APIs are enabled\n"
     ]
    },
    {
     "ename": "RideDemandException",
     "evalue": "There is an error in /tmp/ipykernel_27338/2067020572.py at line 521 with The truth value of a DataFrame is ambiguous. Use a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 521\u001b[39m, in \u001b[36mDataTransformation.push_transformed_data_to_feature_store\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    520\u001b[39m \u001b[38;5;66;03m## inserting new data in the feature group created above\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m \u001b[43mfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moffline\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwait_for_job\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muse_spark\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    523\u001b[39m logger.info(\u001b[33m'\u001b[39m\u001b[33mdata successfully added to hopsworks feature group\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/hsfs/feature_group.py:2996\u001b[39m, in \u001b[36mFeatureGroup.insert\u001b[39m\u001b[34m(self, features, overwrite, operation, storage, write_options, validation_options, wait, transformation_context, transform)\u001b[39m\n\u001b[32m   2991\u001b[39m     warnings.warn(\n\u001b[32m   2992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSpecifying the storage option is not supported if the streaming APIs are enabled\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2993\u001b[39m         stacklevel=\u001b[32m1\u001b[39m,\n\u001b[32m   2994\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2996\u001b[39m feature_dataframe = \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_default_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2998\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m validation_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/hsfs/engine/python.py:750\u001b[39m, in \u001b[36mEngine.convert_to_default_dataframe\u001b[39m\u001b[34m(self, dataframe)\u001b[39m\n\u001b[32m    749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dataframe_copy\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m dataframe == \u001b[33m\"\u001b[39m\u001b[33mspine\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:454\u001b[39m, in \u001b[36mFrameBase.__bool__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__bool__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    455\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe truth value of a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is ambiguous. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    456\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUse a.any() or a.all().\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    457\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: The truth value of a DataFrame is ambiguous. Use a.any() or a.all().",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRideDemandException\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m df = transform.engineer_autoregressive_signals(df)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m#transform.save_data_to_feature_store(df)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpush_transformed_data_to_feature_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 526\u001b[39m, in \u001b[36mDataTransformation.push_transformed_data_to_feature_store\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    523\u001b[39m     logger.info(\u001b[33m'\u001b[39m\u001b[33mdata successfully added to hopsworks feature group\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RideDemandException(e,sys)\n",
      "\u001b[31mRideDemandException\u001b[39m: There is an error in /tmp/ipykernel_27338/2067020572.py at line 521 with The truth value of a DataFrame is ambiguous. Use a.any() or a.all()."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "transform = DataTransformation(transform_config, taxi, weather)\n",
    "df = transform.derive_target_and_join_to_weather_feature()\n",
    "df = transform.engineer_temporal_feature(df)\n",
    "df = transform.city_wide_congestion_features(df)\n",
    "df = transform.zone_level_congestion_features(df)\n",
    "df = transform.citywide_hourly_demand(df)\n",
    "df = transform.generate_neighbor_features(df)\n",
    "df = transform.engineer_autoregressive_signals(df)\n",
    "#transform.save_data_to_feature_store(df)\n",
    "transform.push_transformed_data_to_feature_store(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ec7cc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-09 20:01:43,881 INFO: Initializing external client\n",
      "2026-01-09 20:01:43,884 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2026-01-09 20:01:46,785 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1257642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Specifying the storage option is not supported if the streaming APIs are enabled\n"
     ]
    },
    {
     "ename": "RideDemandException",
     "evalue": "There is an error in /tmp/ipykernel_27338/2152897444.py at line 521 with The truth value of a DataFrame is ambiguous. Use a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 521\u001b[39m, in \u001b[36mDataTransformation.push_transformed_data_to_feature_store\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    520\u001b[39m \u001b[38;5;66;03m## inserting new data in the feature group created above\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m \u001b[43mfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moffline\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwait_for_job\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muse_spark\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    523\u001b[39m logger.info(\u001b[33m'\u001b[39m\u001b[33mdata successfully added to hopsworks feature group\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/hsfs/feature_group.py:2996\u001b[39m, in \u001b[36mFeatureGroup.insert\u001b[39m\u001b[34m(self, features, overwrite, operation, storage, write_options, validation_options, wait, transformation_context, transform)\u001b[39m\n\u001b[32m   2991\u001b[39m     warnings.warn(\n\u001b[32m   2992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSpecifying the storage option is not supported if the streaming APIs are enabled\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2993\u001b[39m         stacklevel=\u001b[32m1\u001b[39m,\n\u001b[32m   2994\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2996\u001b[39m feature_dataframe = \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_default_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2998\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m validation_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/hsfs/engine/python.py:750\u001b[39m, in \u001b[36mEngine.convert_to_default_dataframe\u001b[39m\u001b[34m(self, dataframe)\u001b[39m\n\u001b[32m    749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dataframe_copy\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m dataframe == \u001b[33m\"\u001b[39m\u001b[33mspine\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:454\u001b[39m, in \u001b[36mFrameBase.__bool__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__bool__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    455\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe truth value of a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is ambiguous. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    456\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUse a.any() or a.all().\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    457\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: The truth value of a DataFrame is ambiguous. Use a.any() or a.all().",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRideDemandException\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpush_transformed_data_to_feature_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 526\u001b[39m, in \u001b[36mDataTransformation.push_transformed_data_to_feature_store\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    523\u001b[39m     logger.info(\u001b[33m'\u001b[39m\u001b[33mdata successfully added to hopsworks feature group\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RideDemandException(e,sys)\n",
      "\u001b[31mRideDemandException\u001b[39m: There is an error in /tmp/ipykernel_27338/2152897444.py at line 521 with The truth value of a DataFrame is ambiguous. Use a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "transform.push_transformed_data_to_feature_store(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19fee3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b69e52ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>bin</th>\n",
       "      <th>pickups</th>\n",
       "      <th>datetime</th>\n",
       "      <th>temp</th>\n",
       "      <th>dew</th>\n",
       "      <th>humidity</th>\n",
       "      <th>precip</th>\n",
       "      <th>snow</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>...</th>\n",
       "      <th>neighbor_pickups_sum_x</th>\n",
       "      <th>pickups_lag_1h</th>\n",
       "      <th>pickups_lag_24h</th>\n",
       "      <th>pickups_roll_mean_24h</th>\n",
       "      <th>pickups_roll_std_24h</th>\n",
       "      <th>city_pickups_lag_1h</th>\n",
       "      <th>city_pickups_lag_24h</th>\n",
       "      <th>neighbor_pickups_lag_1h</th>\n",
       "      <th>neighbor_pickups_lag_24h</th>\n",
       "      <th>neighbor_pickups_sum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__null_dask_index__</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201</td>\n",
       "      <td>2025-11-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>53.5</td>\n",
       "      <td>28.6</td>\n",
       "      <td>38.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3204.0</td>\n",
       "      <td>5066.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201</td>\n",
       "      <td>2025-11-01 02:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>02:00:00</td>\n",
       "      <td>52.5</td>\n",
       "      <td>27.1</td>\n",
       "      <td>37.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9349.0</td>\n",
       "      <td>2724.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201</td>\n",
       "      <td>2025-11-01 05:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>05:00:00</td>\n",
       "      <td>50.4</td>\n",
       "      <td>27.1</td>\n",
       "      <td>40.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5217.0</td>\n",
       "      <td>976.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201</td>\n",
       "      <td>2025-11-01 15:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>15:00:00</td>\n",
       "      <td>57.2</td>\n",
       "      <td>29.5</td>\n",
       "      <td>34.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8744.0</td>\n",
       "      <td>6907.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201</td>\n",
       "      <td>2025-11-01 18:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>18:00:00</td>\n",
       "      <td>56.3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>35.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9825.0</td>\n",
       "      <td>6589.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     PULocationID                 bin  pickups  datetime  \\\n",
       "__null_dask_index__                                                        \n",
       "0                             201 2025-11-01 00:00:00        1  00:00:00   \n",
       "1                             201 2025-11-01 02:00:00        0  02:00:00   \n",
       "2                             201 2025-11-01 05:00:00        0  05:00:00   \n",
       "3                             201 2025-11-01 15:00:00        0  15:00:00   \n",
       "4                             201 2025-11-01 18:00:00        1  18:00:00   \n",
       "\n",
       "                     temp   dew  humidity  precip  snow  windspeed  ...  \\\n",
       "__null_dask_index__                                                 ...   \n",
       "0                    53.5  28.6     38.14     0.0   0.0       13.6  ...   \n",
       "1                    52.5  27.1     37.29     0.0   0.0       16.4  ...   \n",
       "2                    50.4  27.1     40.29     0.0   0.0       12.1  ...   \n",
       "3                    57.2  29.5     34.57     0.0   0.0       14.0  ...   \n",
       "4                    56.3  29.0     35.08     0.0   0.0        9.2  ...   \n",
       "\n",
       "                     neighbor_pickups_sum_x  pickups_lag_1h  pickups_lag_24h  \\\n",
       "__null_dask_index__                                                            \n",
       "0                                         0             0.0              0.0   \n",
       "1                                         0             0.0              0.0   \n",
       "2                                         0             0.0              0.0   \n",
       "3                                         0             1.0              0.0   \n",
       "4                                         0             0.0              0.0   \n",
       "\n",
       "                     pickups_roll_mean_24h  pickups_roll_std_24h  \\\n",
       "__null_dask_index__                                                \n",
       "0                                      0.0                   0.0   \n",
       "1                                      0.0                   0.0   \n",
       "2                                      0.0                   0.0   \n",
       "3                                      0.0                   0.0   \n",
       "4                                      0.0                   0.0   \n",
       "\n",
       "                     city_pickups_lag_1h  city_pickups_lag_24h  \\\n",
       "__null_dask_index__                                              \n",
       "0                                 3204.0                5066.0   \n",
       "1                                 9349.0                2724.0   \n",
       "2                                 5217.0                 976.0   \n",
       "3                                 8744.0                6907.0   \n",
       "4                                 9825.0                6589.0   \n",
       "\n",
       "                     neighbor_pickups_lag_1h neighbor_pickups_lag_24h  \\\n",
       "__null_dask_index__                                                     \n",
       "0                                        0.0                      0.0   \n",
       "1                                        0.0                      0.0   \n",
       "2                                        0.0                      0.0   \n",
       "3                                        0.0                      0.0   \n",
       "4                                        0.0                      0.0   \n",
       "\n",
       "                     neighbor_pickups_sum  \n",
       "__null_dask_index__                        \n",
       "0                                       0  \n",
       "1                                       0  \n",
       "2                                       0  \n",
       "3                                       0  \n",
       "4                                       0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_parquet('artifacts/data_transformation/transformed_data.parquet')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ed6d5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PULocationID', 'bin', 'pickups', 'datetime', 'temp', 'dew', 'humidity',\n",
       "       'precip', 'snow', 'windspeed', 'feelslike', 'snowdepth', 'visibility',\n",
       "       'pickup_year', 'pickup_month', 'day_of_month', 'Pickup_hour',\n",
       "       'day_of_week', 'bin_str', 'is_weekend', 'is_rush_hour', 'is_night_hour',\n",
       "       'season_of_year', 'is_holiday', 'Is_special_event', 'is_payday',\n",
       "       'city_avg_speed', 'city_congestion_index', 'zone_avg_speed',\n",
       "       'zone_congestion_index', 'city_pickups', 'neighbor_pickups_sum_x',\n",
       "       'pickups_lag_1h', 'pickups_lag_24h', 'pickups_roll_mean_24h',\n",
       "       'pickups_roll_std_24h', 'city_pickups_lag_1h', 'city_pickups_lag_24h',\n",
       "       'neighbor_pickups_lag_1h', 'neighbor_pickups_lag_24h',\n",
       "       'neighbor_pickups_sum'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "693b9c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int8')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Is_special_event'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42bd05e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data['bin'] = pd.to_datetime(data['bin'])\n",
    "data['pickups'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int8')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['is_holiday'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'neighbor_pickups_sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py:3791\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3790\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3791\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3792\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:152\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:181\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'neighbor_pickups_sum'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mneighbor_pickups_sum\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/frame.py:3893\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3891\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   3892\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m3893\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3894\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   3895\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py:3798\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3793\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3794\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3795\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3796\u001b[39m     ):\n\u001b[32m   3797\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3798\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3799\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3800\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3801\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3802\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3803\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'neighbor_pickups_sum'"
     ]
    }
   ],
   "source": [
    "data['neighbor_pickups_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PULocationID                0\n",
       "bin                         0\n",
       "pickups                     0\n",
       "datetime                    0\n",
       "temp                        0\n",
       "dew                         0\n",
       "humidity                    0\n",
       "precip                      0\n",
       "snow                        0\n",
       "windspeed                   0\n",
       "feelslike                   0\n",
       "snowdepth                   0\n",
       "visibility                  0\n",
       "pickup_year                 0\n",
       "pickup_month                0\n",
       "day_of_month                0\n",
       "Pickup_hour                 0\n",
       "day_of_week                 0\n",
       "bin_str                     0\n",
       "is_weekend                  0\n",
       "is_rush_hour                0\n",
       "is_night_hour               0\n",
       "season_of_year              0\n",
       "is_holiday                  0\n",
       "Is_special_event            0\n",
       "is_payday                   0\n",
       "city_avg_speed              0\n",
       "city_congestion_index       0\n",
       "zone_avg_speed              0\n",
       "zone_congestion_index       0\n",
       "city_pickups                0\n",
       "neighbor_pickups_sum_x      0\n",
       "pickups_lag_1h              0\n",
       "pickups_lag_24h             0\n",
       "pickups_roll_mean_24h       0\n",
       "pickups_roll_std_24h        0\n",
       "city_pickups_lag_1h         0\n",
       "city_pickups_lag_24h        0\n",
       "neighbor_pickups_lag_1h     0\n",
       "neighbor_pickups_lag_24h    0\n",
       "neighbor_pickups_sum        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "225afaf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(191100, 41)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885f0290",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@ensure_annotations\n",
    "def is_holiday(df):\n",
    "    # Weekends\n",
    "    if df['day_of_week'] in [5, 6]:  # Saturday=5, Sunday=6\n",
    "        return 1\n",
    "\n",
    "    # Fixed-date holidays\n",
    "    fixed_holidays = [(1, 1), (7, 4), (11, 11), (6, 19), (12, 25)]\n",
    "    if (df['pickup_month'], df['day_of_month']) in fixed_holidays:\n",
    "        return 1\n",
    "\n",
    "    # Movable holidays\n",
    "    date = pd.Timestamp(year=df['pickup_year'], month=df['pickup_month'], day=df['day_of_month'])\n",
    "\n",
    "    # Last Thursday in November\n",
    "    if df['pickup_month'] == 11 and date.weekday() == 3 and date + pd.Timedelta(\n",
    "        days=7) > pd.Timestamp(year=df['pickup_year'], month=11, day=30):\n",
    "        return 1\n",
    "\n",
    "    # Last Monday in May\n",
    "    if df['pickup_month'] == 5 and date.weekday() == 0 and date + pd.Timedelta(days=7) > pd.Timestamp(\n",
    "        year=df['pickup_year'], month=5, day=31):\n",
    "        return 1\n",
    "\n",
    "    # Third Monday in January\n",
    "    if df['pickup_month'] == 1 and date.weekday() == 0 and 15 <= df['day_of_month'] <= 21:\n",
    "        return 1\n",
    "\n",
    "    # First Monday in September\n",
    "    if df['pickup_month'] == 9 and date.weekday() == 0 and 1 <= df['day_of_month'] <= 7:\n",
    "        return 1\n",
    "\n",
    "    # First Tuesday in November\n",
    "    if df['pickup_month'] == 11 and date.weekday() == 1 and 1 <= df['day_of_month'] <= 7:\n",
    "        return 1\n",
    "\n",
    "    # Second Monday in October\n",
    "    if df['pickup_month'] == 10 and date.weekday() == 0 and 8 <= df['day_of_month'] <= 14:\n",
    "        return 1\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99ee72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os,sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import geopandas as gdp\n",
    "\n",
    "from src.DynamicPricingEngine.logger.logger import logger\n",
    "from src.DynamicPricingEngine.exception.customexception import RideDemandException\n",
    "from src.DynamicPricingEngine.entity.config_entity import DataTransformationConfig\n",
    "from src.DynamicPricingEngine.utils.common_utils import create_dir, load_shapefile_from_zip\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config:DataTransformationConfig, \n",
    "                 nyc_taxi_data:str,\n",
    "                  nyc_weather_data:str):\n",
    "        \n",
    "        self.config = config\n",
    "        taxi_data = pd.read_parquet(nyc_taxi_data)\n",
    "        weather_data = pd.read_csv(nyc_weather_data)\n",
    "\n",
    "        self.taxi_df = taxi_data\n",
    "        self.weather_df = weather_data\n",
    "\n",
    "    def derive_target_and_join_to_weather_feature(self)->pd.DataFrame:\n",
    "        try:\n",
    "\n",
    "            taxi_df = self.taxi_df\n",
    "            weather_df = self.weather_df\n",
    "\n",
    "            ## deriving the target feature\n",
    "            taxi_df['bin'] = taxi_df['tpep_pickup_datetime'].dt.floor('60min')  ## bining into hour\n",
    "\n",
    "            y = (taxi_df\n",
    "                .groupby(['PULocationID','bin'])\n",
    "                .size()\n",
    "                .rename('pickups')\n",
    "                .reset_index())\n",
    "\n",
    "            # Build full grid to include zeros\n",
    "            zones = y['PULocationID'].unique()\n",
    "            time_index = pd.date_range(y['bin'].min(), y['bin'].max(), freq='60min')\n",
    "            grid = pd.MultiIndex.from_product([zones, time_index], names=['PULocationID','bin']).to_frame(index=False)\n",
    "\n",
    "            y = grid.merge(y, how='left', on=['PULocationID','bin']).fillna({'pickups':0})\n",
    "            \n",
    "            logger.info(\"target feature derived successfully\")\n",
    "\n",
    "            ## Adding the weather feature created to the already created pickups columns\n",
    "            weather_df['bin'] = pd.to_datetime(weather_df['day'].astype(str) + ' ' + weather_df['datetime'].astype(str))\n",
    "\n",
    "            #weather_hourly = (weather_df\n",
    "             #               .groupby('bin')\n",
    "              #              .agg({'temp':'mean','dew':'mean','snow':'sum',\n",
    "               #                     'snowdepth':'sum',\n",
    "                #                    'precip':'sum','windspeed':'mean',\n",
    "                 #                   'visibility':'mean','humidity':'mean'})\n",
    "                  #          .reset_index())\n",
    "\n",
    "            df = y.merge(weather_df, on='bin', how='left').ffill()\n",
    "            df.set_index('bin', inplace=True)\n",
    "            logger.info(\"The data joined successfully\")\n",
    "\n",
    "            return df\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(\"Failed to generate the target feature\", e)\n",
    "            raise RideDemandException(e,sys)\n",
    "    \n",
    "    def engineer_temporal_feature(self, df:pd.DataFrame)->pd.DataFrame:\n",
    "        \n",
    "        try:\n",
    "            ##seperating tpep_pickup_datetime\n",
    "            df['pickup_year']= df.index.year\n",
    "            df['pickup_month']= df.index.month\n",
    "            df['day_of_month']= df.index.day\n",
    "            df['Pickup_hour']= df.index.hour\n",
    "            df['day_of_week']= df.index.dayofweek\n",
    "\n",
    "            ## creating the is_weekend, is_rush_hour, is_night_hour, is_holiday, season of the year, and special event data\n",
    "            df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x in [5, 6] else 0)\n",
    "            df['is_rush_hour'] = df['Pickup_hour'].apply(lambda x: 1 if x in [7, 8, 9, 16, 17,18,19] else 0)\n",
    "            df['is_night_hour'] = df['Pickup_hour'].apply(lambda x: 1 if x in [0,1,2,3,4,5,6,20,21,22,23] else 0)\n",
    "\n",
    "            ## creating the season of the year\n",
    "            df['season_of_year'] = df['pickup_month'].apply(lambda x: 'winter' if x in [12, 1, 2]\n",
    "                                                            else 'spring' if x in [3,4,5] else\n",
    "                                                            'summer' if x in [6,7,8] else 'autumn')\n",
    "\n",
    "\n",
    "            ## Deriving the Holdiay feature\n",
    "            def is_holiday(data:pd.DataFrame):\n",
    "                # Weekends\n",
    "                if data['day_of_week'] in [5, 6]:  # Saturday=5, Sunday=6\n",
    "                    return 1\n",
    "\n",
    "                # Fixed-date holidays\n",
    "                fixed_holidays = [(1, 1), (7, 4), (11, 11), (6, 19), (12, 25)]\n",
    "                if (data['pickup_month'], data['day_of_month']) in fixed_holidays:\n",
    "                    return 1\n",
    "\n",
    "                # Movable holidays\n",
    "                date = pd.Timestamp(year=data['pickup_year'], month=data['pickup_month'], day=data['day_of_month'])\n",
    "\n",
    "                # Last Thursday in November\n",
    "                if data['pickup_month'] == 11 and date.weekday() == 3 and date + pd.Timedelta(\n",
    "                    days=7) > pd.Timestamp(year=data['pickup_year'], month=11, day=30):\n",
    "                    return 1\n",
    "\n",
    "                # Last Monday in May\n",
    "                if data['pickup_month'] == 5 and date.weekday() == 0 and date + pd.Timedelta(days=7) > pd.Timestamp(\n",
    "                    year=data['pickup_year'], month=5, day=31):\n",
    "                    return 1\n",
    "\n",
    "                # Third Monday in January\n",
    "                if data['pickup_month'] == 1 and date.weekday() == 0 and 15 <= data['day_of_month'] <= 21:\n",
    "                    return 1\n",
    "\n",
    "                # First Monday in September\n",
    "                if data['pickup_month'] == 9 and date.weekday() == 0 and 1 <= data['day_of_month'] <= 7:\n",
    "                    return 1\n",
    "\n",
    "                # First Tuesday in November\n",
    "                if data['pickup_month'] == 11 and date.weekday() == 1 and 1 <= data['day_of_month'] <= 7:\n",
    "                    return 1\n",
    "\n",
    "                # Second Monday in October\n",
    "                if data['pickup_month'] == 10 and date.weekday() == 0 and 8 <= data['day_of_month'] <= 14:\n",
    "                    return 1\n",
    "\n",
    "                return 0\n",
    "            \n",
    "            df['is_holiday'] = df.apply(is_holiday, axis=1)\n",
    "\n",
    "            ## creating a column for special event\n",
    "            def is_special_event(data):\n",
    "                # Fixed-date holidays\n",
    "                fixed_holidays = [(3, 17), (7, 4), (6, 4), (12,31), (6, 5), (6, 6), (6,7),(6,8),\n",
    "                                (6,9),(6,10), (6,11), (6,12), (6,13),(6,14),(6,15)]\n",
    "                if (data['pickup_month'], data['day_of_month']) in fixed_holidays:\n",
    "                    return 1\n",
    "\n",
    "                # Movable holidays\n",
    "                date = pd.Timestamp(year=data['pickup_year'], month=data['pickup_month'], day=data['day_of_month'])\n",
    "\n",
    "                # Macy's Thanksgiving Parade\n",
    "                if data['pickup_month'] == 11 and date.weekday() == 3 and date + pd.Timedelta(\n",
    "                    days=7) > pd.Timestamp(year=data['pickup_year'], month=11, day=30):\n",
    "                    return 1\n",
    "\n",
    "                # Last Sunday in June (Pride Month)\n",
    "                if data['pickup_month'] == 6 and date.weekday() == 6 and date + pd.Timedelta(days=7) > pd.Timestamp(\n",
    "                    year=data['pickup_year'], month=6, day=30):\n",
    "                    return 1\n",
    "\n",
    "                return 0\n",
    "\n",
    "            df['Is_special_event'] = df.apply(is_special_event, axis=1) ## creating the feature\n",
    "\n",
    "            ## creating a column for Payday Indicator\n",
    "            def is_payday(data):\n",
    "\n",
    "                date = pd.Timestamp(year=data['pickup_year'], month=data['pickup_month'], day=data['day_of_month'])\n",
    "\n",
    "                if date.is_month_start or data['day_of_month']==15 or date.is_month_end:\n",
    "                    return 1\n",
    "\n",
    "                return 0\n",
    "\n",
    "            df['is_payday'] = df.apply(is_payday, axis=1) ##deriving the payday indicator\n",
    "\n",
    "            return df\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise RideDemandException(e,sys)\n",
    "        \n",
    "\n",
    "    def engineer_autoregressive_signals(self, df:pd.DataFrame)->pd.DataFrame:\n",
    "\n",
    "        try:\n",
    "            def make_lags(group, col='pickups'):\n",
    "\n",
    "                ## for lag features\n",
    "                for l in [1,24]:\n",
    "                    group[f'{col}_lag_{l}'] = group[col].shift(l)\n",
    "\n",
    "                ## for rolling mean and std\n",
    "                for w in [24]:\n",
    "                    group[f'{col}_roll_mean_{w}'] = group[col].shift(1).rolling(w).mean()\n",
    "                    group[f'{col}_roll_std_{w}'] = group[col].shift(1).rolling(w).std()\n",
    "                return group\n",
    "            \n",
    "            df.reset_index()\n",
    "            df = df.sort_values(['PULocationID','bin'])\n",
    "            df = df.groupby('PULocationID', group_keys=False).apply(make_lags) ##generating the autoregressive feature\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error('Failed to generate the features', e)\n",
    "            raise RideDemandException(e,sys)\n",
    "        \n",
    "    def city_wide_congestion_features(self, df:pd.DataFrame)-> pd.DataFrame:\n",
    "        try:\n",
    "            taxi_df = self.taxi_df\n",
    "\n",
    "            ## computing the trip distance\n",
    "            taxi_df['trip_duration_hr'] = (\n",
    "                (taxi_df['tpep_dropoff_datetime'] - taxi_df['tpep_pickup_datetime'])\n",
    "                .dt.total_seconds() / 3600\n",
    "            )\n",
    "\n",
    "            ## computing the MPH\n",
    "            taxi_df['MPH']= taxi_df['trip_distance']/taxi_df['trip_duration_hr']\n",
    "\n",
    "            ## filtering out invalid mph\n",
    "            ## based on research, traffic in new york city rarely goes above 60mph\n",
    "            taxi_df = taxi_df[\n",
    "                taxi_df['MPH'].between(1, 60)  # min 1 mph, max 60 mph\n",
    "            ]\n",
    "\n",
    "            ## creating the city-wide congestion\n",
    "            city_speed = (taxi_df\n",
    "                .groupby('bin')['MPH']\n",
    "                .mean()\n",
    "                .rename('city_avg_speed')\n",
    "                .reset_index()\n",
    "            )\n",
    "\n",
    "            ## computing the congestion index by inverting the city_avg_speed\n",
    "            city_speed['city_congestion_index'] = 1 / city_speed['city_avg_speed']\n",
    "\n",
    "            # Merge City speed into the main table\n",
    "            df = df.merge(city_speed, on=['bin'], how='left')\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Unable to generate the city wide features\",e)\n",
    "            raise RideDemandException(e,sys)\n",
    "        \n",
    "\n",
    "    def zone_level_congestion_features(self, df:pd.DataFrame)->pd.DataFrame:\n",
    "        try:\n",
    "            taxi_df = self.taxi_df\n",
    "\n",
    "            ## computing the zone level congestion\n",
    "            zone_speed = (taxi_df\n",
    "                .groupby(['PULocationID', 'bin'])['MPH']\n",
    "                .mean()\n",
    "                .rename('zone_avg_speed')\n",
    "                .reset_index()\n",
    "            )\n",
    "\n",
    "            # Inverse speed as congestion index\n",
    "            zone_speed['zone_congestion_index'] = 1 / zone_speed['zone_avg_speed']\n",
    "\n",
    "            # Build full grid to include zeros\n",
    "            zones = df['PULocationID'].unique()\n",
    "            time_index = pd.date_range(df['bin'].min(), df['bin'].max(), freq='60min')\n",
    "            grid = pd.MultiIndex.from_product([zones, time_index], names=['PULocationID','bin']).to_frame(index=False)\n",
    "\n",
    "            zone_speed = grid.merge(zone_speed, how='left', on=['PULocationID','bin']).fillna({'zone_avg_speed':0, 'zone_congestion_index':0})\n",
    "\n",
    "            # Merge the Zone_speed into the main table\n",
    "            df = df.merge(zone_speed, on=['PULocationID','bin'], how='left')\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error('Unable to generate the zone level features', e)\n",
    "            raise RideDemandException(e,sys)\n",
    "        \n",
    "    def citywide_hourly_demand(self, df:pd.DataFrame)->pd.DataFrame:\n",
    "        try:\n",
    "            # Citywide hourly demand\n",
    "            city_demand = (df.groupby('bin')['pickups']\n",
    "                            .sum()\n",
    "                            .rename('city_pickups')\n",
    "                            .reset_index())\n",
    "\n",
    "            # Merge into zone-hour table\n",
    "            df = df.merge(city_demand, on='bin', how='left')\n",
    "\n",
    "            # Create lag features (1h, 24h)\n",
    "            for lag in [1, 24]:\n",
    "                df[f'city_pickups_lag_{lag}h'] = df['city_pickups'].shift(lag)\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Unable to engineer the citywide hourly demand features\", e)\n",
    "            raise RideDemandException(e,sys)\n",
    "        \n",
    "    def generate_neighbor_features(self, df:pd.DataFrame)->pd.DataFrame:\n",
    "        try:\n",
    "            zones_gdf = load_shapefile_from_zip(self.config.taxi_zone_shapefile_url,\n",
    "                                               self.config.shapefile_dir)\n",
    "            logger.info(f'loaded the shapefile successfully to {self.config.shapefile_dir}')\n",
    "\n",
    "            # Spatial join: each zone with all zones it touches\n",
    "            neighbors_df = gdp.sjoin(zones_gdf, zones_gdf, how=\"left\", predicate=\"touches\")\n",
    "\n",
    "            # Remove self-joins\n",
    "            neighbors_df = neighbors_df[neighbors_df['LocationID_left'] != neighbors_df['LocationID_right']]\n",
    "\n",
    "            # Group into a dictionary: zone -> list of neighbor zones\n",
    "            neighbor_dict = (neighbors_df.groupby('LocationID_left')['LocationID_right']\n",
    "                            .apply(list)\n",
    "                            .to_dict())\n",
    "            \n",
    "            # df: your zone-hour pickup table\n",
    "            # Columns: ['PULocationID', 'bin', 'pickups']\n",
    "            # neighbor_dict: {zone_id: [neighbor_zone_ids, ...]}\n",
    "\n",
    "            # Step 1: Create a DataFrame mapping each zone to its neighbors\n",
    "            neighbor_pairs = []\n",
    "            for zone, neighs in neighbor_dict.items():\n",
    "                for n in neighs:\n",
    "                    neighbor_pairs.append((zone, n))\n",
    "\n",
    "            neighbor_df = pd.DataFrame(neighbor_pairs, columns=['PULocationID', 'neighbor_id'])\n",
    "\n",
    "            # Step 2: Join neighbor_df with df to get neighbor pickups\n",
    "            # Rename df columns for clarity before merge\n",
    "            df_neighbors = df.rename(columns={'PULocationID': 'neighbor_id', 'pickups': 'neighbor_pickups'})\n",
    "            df_neighbors.reset_index(inplace=True)\n",
    "\n",
    "            # Merge: for each (zone, neighbor), bring in neighbor's pickups for each hour\n",
    "            merged = neighbor_df.merge(df_neighbors, on='neighbor_id', how='left')\n",
    "\n",
    "\n",
    "            # Step 3: Group by zone and hour to sum neighbor pickups\n",
    "            neighbor_demand_df = (merged\n",
    "                .groupby(['PULocationID', 'bin'])['neighbor_pickups']\n",
    "                .sum()\n",
    "                .reset_index()\n",
    "                .rename(columns={'neighbor_pickups': 'neighbor_pickups_sum'})\n",
    "            )\n",
    "\n",
    "            # Step 4: Merge back into your main df\n",
    "            df = df.reset_index()\n",
    "            df = df.merge(neighbor_demand_df, on=['PULocationID', 'bin'], how='left')\n",
    "            df['neighbor_pickups_sum'] = df['neighbor_pickups_sum'].fillna(0)\n",
    "\n",
    "            ## computing the Lagged neighbor demand\n",
    "            for lag in [1, 24]:\n",
    "                df[f'neighbor_pickups_lag_{lag}h'] = df.groupby('PULocationID')['neighbor_pickups_sum'].shift(lag)\n",
    "\n",
    "            # Function to sum neighbor demand\n",
    "            #def neighbor_demand(row):\n",
    "             #   neighbors = neighbor_dict\n",
    "              #  neigh_ids = neighbors.get(row['PULocationID'], [])\n",
    "               # mask = (df['PULocationID'].isin(neigh_ids)) & (df['bin'] == row['bin'])\n",
    "                #return df.loc[mask, 'pickups'].sum()\n",
    "\n",
    "            ## handling the missing data caused by lag, rolling mean/std\n",
    "            df = df.fillna(method='bfill')\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Unable to generate the Neighbor features\", e)\n",
    "            raise RideDemandException(e,sys)\n",
    "        \n",
    "    def save_data_to_feature_store(self):\n",
    "        try:\n",
    "            df = self.generate_neighbor_features()\n",
    "            transformed_data_store = self.config.transformed_data_file_path\n",
    "            logger.info(f\"Saving the transformed dataset to the feature store\")\n",
    "\n",
    "            df.to_csv(transformed_data_store, index=False)\n",
    "            logger.info(f'Transformed data saved to path, {transformed_data_store}')\n",
    "\n",
    "            print(f\"size of Transformed data: {df.shape}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Unable to save the file\",e)\n",
    "            raise RideDemandException(e,sys)\n",
    "    \n",
    "    #def feature_engineering(self):\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6f79ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import timedelta, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7113473f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start date: 2025-11-01, end date: 2025-11-30\n"
     ]
    }
   ],
   "source": [
    "now = (datetime.today())#- relativedelta(months=1))\n",
    "# end_date = datetime.strptime(now, \"%Y-%m-%d\") ## converting to datetime\n",
    "end_date = now - timedelta(days=now.day) ## retrieving the last day of the previous month\n",
    "\n",
    "## accessing the previous month\n",
    "days_to_subtract = time_subtract(end_date.strftime('%Y-%m-%d'))\n",
    "end_date = (end_date - timedelta(days=days_to_subtract))\n",
    "\n",
    "## start of the month\n",
    "days= time_subtract(end_date.strftime('%Y-%m-%d'))\n",
    "start_date= end_date - timedelta(days=days-1)\n",
    "\n",
    "start_date = start_date.strftime('%Y-%m-%d')\n",
    "end_date = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "print(f'start date: {start_date}, end date: {end_date}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed53ca32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from src.DynamicPricingEngine.utils.data_transformation_utils import DataPusher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a758daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"#DataPusher()\n",
    "df = data._reliable_insert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d94e6d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcdac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, field_validator\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# 1. Improved Pydantic Model\n",
    "class PredictionRequest(BaseModel):\n",
    "    datetime_val: str # Renamed from 'datetime' to avoid confusion with module\n",
    "    pulocationid: int\n",
    "\n",
    "    @field_validator('datetime_val', mode='before')\n",
    "    @classmethod\n",
    "    def parse_datetime(cls, v):\n",
    "        return pd.to_datetime(v)\n",
    "\n",
    "    @field_validator('pulocationid')\n",
    "    @classmethod\n",
    "    def check_pulocationid(cls, v):\n",
    "        if not (1 <= v <= 263):\n",
    "            raise ValueError(\"PULocationID must be between 1-263\")\n",
    "        return v\n",
    "\n",
    "class InferencePipeline:\n",
    "    # ... (init logic is mostly fine, but ensure you pass the Hopsworks FS object) ...\n",
    "    def __init__(self, config, start_date_time: str, \n",
    "                 end_date_time:str, api_key: str, pulocationid: int):\n",
    "        try:\n",
    "            self.config = config\n",
    "            if not isinstance(start_date_time, str):\n",
    "                raise ValueError(\"start_date_time must be a string in 'YYYY-MM-DD' format\")\n",
    "            self.date = datetime.now()\n",
    "\n",
    "            if not isinstance(end_date_time, str):\n",
    "                raise ValueError(\"end_date_time must be a string in 'YYYY-MM-DD' format\")\n",
    "            self.end_date = end_date_time\n",
    "\n",
    "            self.api_key = api_key\n",
    "\n",
    "            if not (1<=pulocationid<=263):\n",
    "                raise ValueError(\"PULocationID must be between 1-263\")\n",
    "            self.pulocationid = pulocationid\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Error initializing Inference Pipeline\", e)\n",
    "            raise RideDemandException(e, sys)\n",
    "\n",
    "    def get_nyc_prediction_weather_data(self)-> pd.DataFrame:\n",
    "\n",
    "        try:\n",
    "            base_url = self.config.weather_data_url\n",
    "            location: str = \"New York, NY, United States\"\n",
    "            start_of_month_str = self.start_date\n",
    "            end_of_month_str = self.end_date\n",
    "            api_key = self.api_key\n",
    "\n",
    "            params = {\n",
    "                \"unitGroup\": \"us\",\n",
    "                \"key\": api_key,\n",
    "                \"include\": \"days,hours,current,alerts,stations\",\n",
    "                \"contentType\": \"json\"\n",
    "            }\n",
    "\n",
    "            url = f\"{base_url}/{location}/{start_of_month_str}/{end_of_month_str}\"\n",
    "            logger.info(f\"Fetching data from {start_of_month_str} to {end_of_month_str}...\")\n",
    "\n",
    "            try:\n",
    "                response = requests.get(url, params=params)\n",
    "                response.raise_for_status()\n",
    "\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"Request failed: {e}\")\n",
    "                return None\n",
    "\n",
    "            data = response.json()\n",
    "            days = data.get(\"days\", [])\n",
    "            if not days:\n",
    "                print(\"No 'days' data found in response.\")\n",
    "                return None\n",
    "\n",
    "            hourly_records = []\n",
    "            fields = ['datetime', 'temp', 'humidity']\n",
    "\n",
    "            for day in days:\n",
    "                for hour in day.get(\"hours\", []):\n",
    "                    filtered_hour = {key: hour.get(key) for key in fields}\n",
    "                    filtered_hour[\"day\"] = day.get(\"datetime\")\n",
    "                    hourly_records.append(filtered_hour)\n",
    "\n",
    "            df_hours = pd.DataFrame(hourly_records)\n",
    "            logger.info(f\"Retrieved {len(days)} days ({df_hours.shape[0]} hourly records).\")\n",
    "\n",
    "            return df_hours\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"failed to extract weather data {e}\")\n",
    "            raise RideDemandException(e,sys)\n",
    "\n",
    "    def engineer_temporal_prediction_features(self, df: pd.DataFrame)-> pd.DataFrame:\n",
    "        try:\n",
    "            pulocationid = self.pulocationid\n",
    "            df['datetime']= pd.to_datetime(df['day'] + ' ' + df['datetime'])\n",
    "            df.drop(columns=['day'], inplace=True)\n",
    "\n",
    "            # Extract temporal features\n",
    "            df['pickup_hour'] = df['datetime'].dt.hour\n",
    "            df['is_rush_hour'] = df['hour'].apply(lambda x: 1 if 7 <= x <= 9 or 16 <= x <= 19 else 0)\n",
    "            df['pulocationid'] = pulocationid\n",
    "\n",
    "            logger.info(\"Temporal Feature for prediction generated successfully.\")\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"failed to engineer temporal features for prediction data {e}\")\n",
    "            raise RideDemandException(e,sys)\n",
    "\n",
    "    def generate_lag_features_for_prediction(self, weather_pred_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        FIXED: You must fetch historical demand from Hopsworks to create lags.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 1. Connect to Hopsworks Feature Store (Assumes self.fs is initialized)\n",
    "            # You need historical demand for the specific location\n",
    "            fg = self.fs.get_feature_group(name=\"taxi_demand_feature_group\", version=1)\n",
    "            \n",
    "            # 2. Fetch last 24-48 hours of ACTUAL demand to create lags for the NEW prediction\n",
    "            # Use hopsworks query or read_feature_group\n",
    "            hist_df = fg.filter(fg.pulocationid == self.pulocationid).read()\n",
    "            hist_df = hist_df.sort_values('datetime')\n",
    "\n",
    "            # 3. Combine with your weather_pred_df to ensure continuous timeline\n",
    "            full_df = pd.concat([hist_df, weather_pred_df]).sort_values('datetime')\n",
    "\n",
    "            # 4. Generate actual lags on the 'target' column (e.g., 'ride_count')\n",
    "            full_df['pickup_lag1'] = full_df['ride_count'].shift(1)\n",
    "            full_df['pickup_lag24'] = full_df['ride_count'].shift(24)\n",
    "            \n",
    "            full_df['pickup_roll_mean24'] = full_df['ride_count'].shift(1).rolling(window=24).mean()\n",
    "\n",
    "            # 5. Return only the rows that belong to your prediction window\n",
    "            return full_df[full_df['datetime'] >= weather_pred_df['datetime'].min()]\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to generate lag features: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b28ddbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "540f6fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nyc_prediction_weather_data(weather_data_url):\n",
    "\n",
    "    try:\n",
    "        #base_url = weather_data_url\n",
    "        location: str = \"New York, NY, United States\"\n",
    "        base_url = f\"https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/{location}/today\"\n",
    "        #api_key = self.api_key\n",
    "\n",
    "        params = {\n",
    "            \"unitGroup\": \"us\",\n",
    "            \"key\": 'HT2ZYDZG8J25XYFP2E9ABUXJF',\n",
    "            \"include\": \"hours,current,alerts\",\n",
    "            \"contentType\": \"json\"\n",
    "        }\n",
    "\n",
    "        url = f\"{base_url}/{location}/\"\n",
    "        logger.info(f\"Fetching weather data ...\")\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            response.raise_for_status()\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            return None\n",
    "\n",
    "        data = response.json()\n",
    "        #days = data.get(\"days\", [])\n",
    "        #if not days:\n",
    "         #   print(\"No 'days' data found in response.\")\n",
    "          #  return None\n",
    "        \n",
    "        # Access specific data (e.g., current conditions)\n",
    "        current = data.get(\"currentConditions\")\n",
    "        print(f\"Current temp in NYC: {current.get('temp')}Â°F\")\n",
    "\n",
    "        hourly_records = []\n",
    "        #fields = ['datetime', 'temp', 'humidity']\n",
    "\n",
    "        #for day in days:\n",
    "         #   for hour in day.get(\"hours\", []):\n",
    "          #      filtered_hour = {key: hour.get(key) for key in fields}\n",
    "           #     filtered_hour[\"day\"] = day.get(\"datetime\")\n",
    "        hourly_records.append(current)\n",
    "\n",
    "        df_hours = pd.DataFrame(hourly_records)\n",
    "        #logger.info(f\"Retrieved {len(days)} days ({df_hours.shape[0]} hourly records).\")\n",
    "\n",
    "        return df_hours\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"failed to extract weather data {e}\")\n",
    "        raise RideDemandException(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948e49f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def nyc_current_weather_data():\n",
    "    # Define your API configuration\n",
    "    API_KEY = \"\"  # Replace with your actual key\n",
    "    LOCATION = \"New York, NY, United States\"\n",
    "    BASE_URL = f\"https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline\"\n",
    "    \n",
    "    \n",
    "    # Construct the full URL\n",
    "    url = f\"{BASE_URL}/{LOCATION}/today\"\n",
    "\n",
    "    # Set up the query parameters\n",
    "    params = {\n",
    "        \"unitGroup\": \"us\",\n",
    "        \"include\": \"current\",\n",
    "        \"key\": API_KEY,\n",
    "        \"contentType\": \"json\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Execute the GET request\n",
    "        response = requests.get(url, params=params)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse the JSON data\n",
    "        data = response.json()\n",
    "        \n",
    "        # Access specific data (e.g., current conditions)\n",
    "        current = data.get(\"currentConditions\")\n",
    "        hourly_records = []\n",
    "        hourly_records.append(current)\n",
    "\n",
    "        df_hours = pd.DataFrame(hourly_records)\n",
    "        df_hours = df_hours[['datetime', 'temp', 'humidity']]\n",
    "        df_hours['datetime'] = pd.to_datetime(df_hours['datetime'])\n",
    "        \n",
    "        return df_hours\n",
    "        \n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        print(f\"HTTP error occurred: {err}\")\n",
    "    except Exception as err:\n",
    "        print(f\"An error occurred: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f71431c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63979a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.DynamicPricingEngine.logger.logger import logger\n",
    "from src.DynamicPricingEngine.exception.customexception import RideDemandException\n",
    "import os, sys\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11181edd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b991fe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsfs.feature import Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bee74be",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"\"\n",
    "#config= config\n",
    "cat_cols = ['pickup_hour','is_rush_hour']\n",
    "\n",
    "def retrieve_engineered_feature():\n",
    "    try:\n",
    "        now = datetime.today()\n",
    "        end_date = now - timedelta(days=now.day) ## retrieving the last day of the previous month\n",
    "\n",
    "        ## accessing the previous month\n",
    "        days_to_subtract = time_subtract(end_date.strftime('%Y-%m-%d'))\n",
    "        end_date = (end_date- timedelta(days=days_to_subtract)+ timedelta(days=1))\n",
    "\n",
    "        ## a year back from end date \n",
    "        start_date = end_date - relativedelta(months= 1)\n",
    "\n",
    "        start_date = start_date.strftime('%Y-%m-%d')\n",
    "        end_date = (end_date-relativedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "        logger.info('Retrieving the dataset from hopsworks feature store')\n",
    "\n",
    "        ## login to feature store\n",
    "        #project = hopsworks.login(project='RideDemandPrediction', api_key_value=api_key)\n",
    "        project = hopsworks.login(api_key_value=api_key)\n",
    "        fs = project.get_feature_store()\n",
    "\n",
    "        # Get the feature group\n",
    "        fg = fs.get_feature_group(name=\"ridedemandprediction\", version=1)\n",
    "\n",
    "        features = [\n",
    "            'bin',\n",
    "            'temp',\n",
    "            'humidity',\n",
    "            'pickup_hour',\n",
    "            'is_rush_hour',\n",
    "            'city_avg_speed',\n",
    "            'zone_avg_speed',\n",
    "            'zone_congestion_index',\n",
    "            'pickups_lag_1h',\n",
    "            'pulocationid',\n",
    "            'pickups_lag_24h',\n",
    "            'city_pickups_lag_1h',\n",
    "            'neighbor_pickups_lag_1h',\n",
    "            'pickups',\n",
    "            'datetime'\n",
    "        ]\n",
    "\n",
    "        query=fg.select(features).filter((Feature('bin_str') >= start_date) & (Feature('bin_str') <= end_date))\n",
    "\n",
    "        # creating a feature view\n",
    "        df = query.read()\n",
    "        logger.info('Feature data retrieved successfully from the feature store')\n",
    "    \n",
    "        df.set_index(['bin'], inplace=True)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error retrieving the dataset, {e}\")\n",
    "        raise RideDemandException(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "af445044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsfs.feature_view import FeatureView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "100c56e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_engineered_feature():\n",
    "        try:\n",
    "            ## login to feature store\n",
    "            \n",
    "            now = datetime.today()\n",
    "            end_date = now - timedelta(days=now.day) ## retrieving the last day of the previous month\n",
    "\n",
    "            ## accessing the previous month\n",
    "            days_to_subtract = time_subtract(end_date.strftime('%Y-%m-%d'))\n",
    "            end_date = (end_date- timedelta(days=days_to_subtract)+ timedelta(days=1))\n",
    "\n",
    "            ## a year back from end date \n",
    "            start_date = end_date - relativedelta(months= 1)\n",
    "\n",
    "            start_date = start_date.strftime('%Y-%m-%d')\n",
    "            end_date = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "            logger.info('Retrieving the dataset from hopsworks feature store')\n",
    "\n",
    "\n",
    "            ## login to feature store\n",
    "            project = hopsworks.login(project='RideDemandPrediction', api_key_value=api_key)\n",
    "            fs = project.get_feature_store()\n",
    "\n",
    "            # Get the feature group\n",
    "            fg = fs.get_feature_group(name=\"ridedemandprediction\", version=1)\n",
    "            query=fg.select([\n",
    "                'bin',\n",
    "                'temp',\n",
    "                'humidity',\n",
    "                'pickup_hour',\n",
    "                'is_rush_hour',\n",
    "                'city_avg_speed',\n",
    "                'zone_avg_speed',\n",
    "                'zone_congestion_index',\n",
    "                'pickups_lag_1h',\n",
    "                'pulocationid',\n",
    "                'pickups_lag_24h',\n",
    "                'city_pickups_lag_1h',\n",
    "                'neighbor_pickups_lag_1h',\n",
    "                'pickups'\n",
    "            ])\n",
    "\n",
    "            # delete the previous month feature view data\n",
    "            FeatureView.clean(feature_store_id=fs._id, \n",
    "                                feature_view_name='ride_demand_fv',\n",
    "                                feature_view_version=1)\n",
    "\n",
    "            # create a new feature view from the feature group\n",
    "            feature_view = fs.create_feature_view(name=\"ride_demand_fv\",\n",
    "                                                    version=1,\n",
    "                                                    description=\"Features for ride demand prediction\",\n",
    "                                                    query=query)\n",
    "\n",
    "            logger.info('hopsworks feature view created successfully')\n",
    "\n",
    "            # Materialize training dataset using Spark job\n",
    "            version, jobs = feature_view.create_training_data(start_time = start_date,\n",
    "                                                                end_time = end_date,\n",
    "                                                                description=\"365 days ride demand training data\",\n",
    "                                                                data_format=\"parquet\",\n",
    "                                                                write_options = {'use_spark': True}\n",
    "                                                                )\n",
    "\n",
    "            logger.info('Training data created successfully and materialized in hopsworks')\n",
    "            logger.info(f\"Data from {start_date} to {end_date} created and materialized Successfully\")\n",
    "\n",
    "            df, _ = feature_view.get_training_data(training_dataset_version=1,\n",
    "                                                read_options={\"use_hive\":False})\n",
    "            \n",
    "            logger.info('Data successfully retrieved from the feature store')\n",
    "            \n",
    "            df.set_index(['bin'], inplace=True)\n",
    "\n",
    "            return df\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error retrieving the dataset, {e}\")\n",
    "            raise RideDemandException(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14acecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_engineered_feature(self):\n",
    "        try:\n",
    "            ## login to feature store\n",
    "            now = datetime.today()\n",
    "            end_date = now - timedelta(days=now.day) ## retrieving the last day of the previous month\n",
    "\n",
    "            ## accessing the previous month\n",
    "            days_to_subtract = time_subtract(end_date.strftime('%Y-%m-%d'))\n",
    "            end_date = (end_date- timedelta(days=days_to_subtract)+ timedelta(days=1))\n",
    "\n",
    "            ## a year back from end date \n",
    "            start_date = end_date - relativedelta(months= 1)\n",
    "\n",
    "            start_date = start_date.strftime('%Y-%m-%d')\n",
    "            end_date = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "            logger.info('Retrieving the dataset from hopsworks feature store')\n",
    "\n",
    "\n",
    "            ## login to feature store\n",
    "            project = hopsworks.login(project='RideDemandPrediction', api_key_value=self.api_key,\n",
    "                                  #    config={'connect_timeout': 300, 'read_timeout': 300} \n",
    "                                        )\n",
    "            fs = project.get_feature_store()\n",
    "\n",
    "            # Get the feature group\n",
    "            #fg = fs.get_feature_group(name=\"ridedemandprediction\", version=1)\n",
    "            #query=fg.select_all()\n",
    "            \n",
    "            feature_view = fs.get_feature_view(name=\"ride_demand_fv\",\n",
    "                                                    version=1)\n",
    "\n",
    "            #logger.info('hopsworks feature view created successfully')\n",
    "\n",
    "            try:\n",
    "                feature_view.delete_training_dataset(training_dataset_version=1)\n",
    "                logger.info(\"Training data version 1 deleted.\")\n",
    "\n",
    "                # Materialize training dataset using Spark job\n",
    "                version, jobs = feature_view.create_training_data(start_time = start_date,\n",
    "                                                                    end_time = end_date,\n",
    "                                                                    description=\"365 days ride demand training data\",\n",
    "                                                                    data_format=\"parquet\",\n",
    "                                                                    write_options = {'use_spark': True}\n",
    "                                                                    )\n",
    "\n",
    "                logger.info('Training data created successfully and materialized in hopsworks')\n",
    "                logger.info(f\"Data from {start_date} to {end_date} created and materialized Successfully\")\n",
    "\n",
    "\n",
    "            except:\n",
    "                # Materialize training dataset using Spark job\n",
    "                version, jobs = feature_view.create_training_data(start_time = start_date,\n",
    "                                                                    end_time = end_date,\n",
    "                                                                    description=\"365 days ride demand training data\",\n",
    "                                                                    data_format=\"parquet\",\n",
    "                                                                    write_options = {'use_spark': True}\n",
    "                                                                    )\n",
    "\n",
    "                logger.info('Training data created successfully and materialized in hopsworks')\n",
    "                logger.info(f\"Data from {start_date} to {end_date} created and materialized Successfully\")\n",
    "\n",
    "            df, _ = feature_view.get_training_data(training_dataset_version=1,\n",
    "                                                read_options={\"use_hive\":False})\n",
    "            \n",
    "            logger.info('Data successfully retrieved from the feature store')\n",
    "            \n",
    "            df.set_index(['bin'], inplace=True)\n",
    "\n",
    "            return df\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error retrieving the dataset, {e}\")\n",
    "            raise RideDemandException(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85b3d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_historical_pickup_data()-> pd.DataFrame:\n",
    "        try:\n",
    "            cat_cols = ['pickup_hour','is_rush_hour']\n",
    "\n",
    "            # Define the time range for historical data extraction\n",
    "            now = datetime.now()\n",
    "            end_date = datetime.now() - timedelta(minutes=now.minute, \n",
    "                                                  seconds=now.second, \n",
    "                                                  microseconds=now.microsecond)\n",
    "            start_date = end_date - timedelta(hours=24)   \n",
    "\n",
    "            logger.info('Retrieving the dataset from hopsworks feature store')\n",
    "\n",
    "            ## login to feature store\n",
    "            project = hopsworks.login(project='RideDemandPrediction', api_key_value=api_key,\n",
    "                                    )\n",
    "            fs = project.get_feature_store()\n",
    "\n",
    "            # Get the feature group\n",
    "            fg = fs.get_feature_group(name=\"ridedemandprediction\", version=1)\n",
    "\n",
    "            final_features = [\n",
    "                'temp',\n",
    "                'humidity',\n",
    "                'pickup_hour',\n",
    "                'is_rush_hour',\n",
    "                'city_avg_speed',\n",
    "                'zone_avg_speed',\n",
    "                'zone_congestion_index',\n",
    "                'pickups_lag_1h',\n",
    "                'pulocationid',\n",
    "                'pickups_lag_24h',\n",
    "                'city_pickups_lag_1h',\n",
    "                'neighbor_pickups_lag_1h',\n",
    "                'pickups'\n",
    "            ]\n",
    "            query=fg.select(final_features)\n",
    "\n",
    "                ## login to feature store\n",
    "            project = hopsworks.login(project='RideDemandPrediction', api_key_value=api_key)\n",
    "            fs = project.get_feature_store()\n",
    "\n",
    "            # Get the feature group\n",
    "            #fg = fs.get_feature_group(name=\"ridedemandprediction\", version=1)\n",
    "            #query=fg.select_all()\n",
    "            \n",
    "            feature_view = fs.create_feature_view(name=\"ride_demand_fv\",\n",
    "                                                    version=1)\n",
    "\n",
    "            #logger.info('hopsworks feature view created successfully')\n",
    "\n",
    "            # creating a feature view\n",
    "            # create a new feature view from the feature group\n",
    "            #feature_view = fs.create_feature_view(name=\"ride_demand_prediction_fv\",\n",
    "                                                   # version=1,\n",
    "                                                  #  description=\"ride demand historical data for prediction\",\n",
    "                                                 #   query=query)\n",
    "\n",
    "            logger.info('hopsworks feature view created successfully')\n",
    "\n",
    "            feature_view = fs.get_feature_view(name='ride_demand_prediction_fv', version= 1)\n",
    "\n",
    "            # Materialize training dataset using Spark job\n",
    "            version, jobs = feature_view.create_training_data(start_time = start_date,\n",
    "                                                                end_time = end_date,\n",
    "                                                                description=\"ride demand training data\",\n",
    "                                                                data_format=\"parquet\",\n",
    "                                                                write_options = {'use_spark': True}\n",
    "                                                                )\n",
    "\n",
    "            logger.info('Training data created successfully and materialized in hopsworks')\n",
    "            logger.info(f\"Data from {start_date} to {end_date} created and materialized Successfully\")\n",
    "\n",
    "            feature_view = fs.get_feature_view(name='ride_demand_fv', version= 1)\n",
    "\n",
    "            df, _ = feature_view.get_training_data(training_dataset_version=1,\n",
    "                                                read_options={\"use_hive\":False})\n",
    "            \n",
    "            logger.info('Data successfully retrieved from the feature store')\n",
    "            \n",
    "            df.set_index(['bin'], inplace=True)\n",
    "\n",
    "            return df\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"failed to extract historical pickup data {e}\")\n",
    "            raise RideDemandException(e, sys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93d344b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emmanuel/Desktop/Dynamic-Pricing-Engine/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import hopsworks\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import sys\n",
    "from hsfs.feature import Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5dcaf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leap_year(year:int)->bool:\n",
    "\n",
    "    try:\n",
    "\n",
    "        if (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "\n",
    "def time_subtract(date:str)->int:\n",
    "    \"\"\"\n",
    "    A function to calculate the number of days to subract from the data ingesting\n",
    "    Args:\n",
    "        date (str): the date to use\n",
    "    Return:\n",
    "        days(int): the number of days calculated\n",
    "       \"\"\"\n",
    "    try:\n",
    "\n",
    "        date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "        days = 0\n",
    "\n",
    "        if (date.month != 2) and (date.month not in (4,9,6,11)):\n",
    "            days=31\n",
    "\n",
    "        elif (date.month != 2) and (date.month in (4,9,6,11)):\n",
    "            days=30\n",
    "\n",
    "        elif (date.month == 2) and (leap_year(date.year)):\n",
    "            days=29\n",
    "\n",
    "        else:\n",
    "            days=28\n",
    "\n",
    "        return days\n",
    "\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83db8614",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a92f153b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def extract_historical_pickup_data()-> pd.DataFrame:\n",
    "    try:\n",
    "      ## login to feature store\n",
    "      project = hopsworks.login(project='RideDemandPrediction', api_key_value=api_key)\n",
    "      fs = project.get_feature_store()\n",
    "\n",
    "      final_features = [\n",
    "              'temp', 'humidity', 'pickup_hour', 'is_rush_hour', 'city_avg_speed', 'zone_avg_speed',\n",
    "              'zone_congestion_index', 'pickups_lag_1h', 'pulocationid', 'pickups_lag_24h', 'city_pickups_lag_1h',\n",
    "              'neighbor_pickups_lag_1h', 'neighbor_pickups_sum', 'pickups', 'bin_str', 'bin']\n",
    "\n",
    "      ## creating a new feature group\n",
    "      prediction_fg = fs.create_feature_group(\n",
    "          name=\"demand_predictions\",\n",
    "          version=1,\n",
    "          primary_key=['pulocationid', 'bin_str'],\n",
    "          event_time='bin',\n",
    "          online_enabled=False,\n",
    "          description=\"Logs of model predictions for evaluation\"\n",
    "        )\n",
    "      print('feature group created successfully')\n",
    "\n",
    "      # Defining the time range for historical data extraction\n",
    "      now = datetime.now().replace(hour=0,minute=0, second=0, microsecond=0)\n",
    "      end_date = now - timedelta(days=now.day) ## retrieving the last day of the previous month\n",
    "\n",
    "      ## accessing the previous month\n",
    "      days_to_subtract = time_subtract(end_date.strftime('%Y-%m-%d'))\n",
    "      end_date = (end_date- timedelta(days=days_to_subtract)+ timedelta(days=1))\n",
    "\n",
    "      ## a year back from end date\n",
    "      start_date = end_date - relativedelta(days= 1)\n",
    "\n",
    "      start_date = start_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "      end_date = end_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "      print(start_date, end_date)\n",
    "\n",
    "      print('Retrieving the dataset from hopsworks feature store')\n",
    "\n",
    "      # Get the feature group\n",
    "      train_fg = fs.get_feature_group(name=\"ridedemandprediction\", version=1)\n",
    "      query=train_fg.select(final_features).filter(train_fg.bin >= start_date).filter(train_fg.bin <= end_date)\n",
    "      historic_df = query.read()\n",
    "\n",
    "      print('data successfully retrieved from the feature store')\n",
    "\n",
    "      prediction_fg.insert(historic_df, storage = 'offline', write_options = {'wait_for_job': True, 'use_spark':True})\n",
    "      print('data successfuly inserted')\n",
    "\n",
    "      return historic_df\n",
    "\n",
    "    except Exception as e:\n",
    "      print(f\"failed to extract historical pickup data {e}\")\n",
    "      raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ae01e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-30 00:00:00 2025-12-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Defining the time range for historical data extraction\n",
    "now = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "end_date = now - timedelta(days=now.day) ## retrieving the last day of the previous month\n",
    "\n",
    "## accessing the previous month\n",
    "days_to_subtract = time_subtract(end_date.strftime('%Y-%m-%d'))\n",
    "end_date = (end_date- timedelta(days=days_to_subtract)+ timedelta(days=1))\n",
    "\n",
    "## a year back from end date\n",
    "start_date = end_date - relativedelta(days= 1)\n",
    "\n",
    "start_date = start_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "end_date = end_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(start_date, end_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "add1643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.DynamicPricingEngine.logger.logger import logger\n",
    "from src.DynamicPricingEngine.exception.customexception import RideDemandException\n",
    "import hopsworks\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85543dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nyc_prediction_weather_data()-> pd.DataFrame:\n",
    "\n",
    "    try:\n",
    "        # Define your API configuration\n",
    "        ny_tz = ZoneInfo(\"America/New_York\")\n",
    "        api_key = \"\"\n",
    "        location = \"New York, NY, United States\"\n",
    "        base_url = f\"https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline\"\n",
    "    \n",
    "        params = {\n",
    "            \"unitGroup\": \"us\",\n",
    "            \"key\": api_key,\n",
    "            \"include\": \"current\",\n",
    "            \"contentType\": \"json\"\n",
    "        }\n",
    "\n",
    "        url = f\"{base_url}/{location}/today\"\n",
    "        logger.info(f\"Fetching data for current timestamp {datetime.now(ny_tz).strftime('%Y-%m-%d, %H')}:00:00 ...\")\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            response.raise_for_status()\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            return None\n",
    "\n",
    "        # Access specific data (e.g., current conditions)\n",
    "        response = response.json()\n",
    "        current = response.get(\"currentConditions\")\n",
    "        hourly_records = []\n",
    "        hourly_records.append(current)\n",
    "\n",
    "        df_hours = pd.DataFrame(hourly_records)\n",
    "        df_hours = df_hours[['datetime', 'temp', 'humidity']]\n",
    "        df_hours['datetime'] = pd.to_datetime(df_hours['datetime']).dt.floor('H')+ timedelta(hours=1)\n",
    "\n",
    "        logger.info(f\"Successfully Retrieved the weather data for {datetime.now(ny_tz).strftime('%Y-%m-%d, %H')}:00:00.\")\n",
    "\n",
    "        return df_hours\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"failed to extract weather data {e}\")\n",
    "        raise RideDemandException(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "982a665e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_temporal_prediction_features(weather_df: pd.DataFrame)-> pd.DataFrame:\n",
    "    try:\n",
    "        #pulocationid = self.pulocationid\n",
    "        # Extract temporal features\n",
    "        weather_df['pickup_hour'] = weather_df['datetime'].dt.hour\n",
    "        #weather_df['day_of_week'] = weather_df['datetime'].dt.dayofweek\n",
    "        weather_df['is_rush_hour'] = weather_df['pickup_hour'].apply(lambda x: 1 if 7 <= x <= 9 or 16 <= x <= 19 else 0)\n",
    "      #  weather_df['pulocationid'] = pulocationid\n",
    "        weather_df.rename(columns={'datetime':'bin'}, inplace=True)\n",
    "\n",
    "        \n",
    "        logger.info(\"Temporal Feature for prediction generated successfully.\")\n",
    "        return weather_df\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"failed to engineer temporal features for prediction data {e}\")\n",
    "        raise RideDemandException(e,sys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b66e4054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_historical_pickup_data(start_date=None, end_date=None) -> pd.DataFrame:\n",
    "    try:\n",
    "        project = hopsworks.login(project='RideDemandPrediction', api_key_value=api_key)\n",
    "        fs = project.get_feature_store()\n",
    "\n",
    "        ny_tz = ZoneInfo(\"America/New_York\")\n",
    "        now_ny = datetime.now(ny_tz).replace(minute=0, second=0, microsecond=0)\n",
    "\n",
    "        start_date_utc = (now_ny - relativedelta(hours=24))\n",
    "        end_date_utc = now_ny\n",
    "\n",
    "        fh_start = start_date_utc.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        fh_end = end_date_utc.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        prediction_fg = fs.get_or_create_feature_group(\n",
    "            name=\"demand_predictions\",\n",
    "            version=1,\n",
    "            primary_key=['pulocationid', 'bin_str'],\n",
    "            event_time='bin',\n",
    "            description=\"Logs of model predictions for evaluation\"\n",
    "        )\n",
    "        if start_date is None or end_date is None:\n",
    "          start_date =fh_start\n",
    "          end_date = fh_end\n",
    "\n",
    "        final_features = [\n",
    "              'temp', 'humidity', 'pickup_hour', 'is_rush_hour', 'city_avg_speed', 'zone_avg_speed',\n",
    "              'zone_congestion_index', 'pickups_lag_1h', 'pulocationid', 'pickups_lag_24h', 'city_pickups_lag_1h',\n",
    "              'neighbor_pickups_lag_1h', 'neighbor_pickups_sum', 'pickups', 'bin_str', 'bin']\n",
    "\n",
    "        # Use the explicit query format\n",
    "        query = prediction_fg.select(final_features).filter(\n",
    "            (prediction_fg.bin >= start_date) & (prediction_fg.bin <= end_date)\n",
    "        )\n",
    "\n",
    "        historic_df = query.read()\n",
    "\n",
    "        print(f\"Successfully retrieved {len(historic_df)} rows for window: {start_date} to {end_date}\")\n",
    "\n",
    "        return historic_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract historical pickup data: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lag_features_for_prediction(self, hist_df:pd.DataFrame,\n",
    "                                              pred_df: pd.DataFrame)-> pd.DataFrame:\n",
    "        try:\n",
    "            #merging historical and prediction data\n",
    "            df = pd.concat([hist_df, pred_df], axis = 0, ignore_index=True)\n",
    "            df = df.sort_values(by='datetime')\n",
    "            df.set_index('datetime', inplace=True)\n",
    "\n",
    "            # getting only the last 24 hours of historical data\n",
    "            df = df.last('24H')\n",
    "\n",
    "            ## creating citywide pickup counts\n",
    "            df_city = df.groupby('datetime').agg({'pickups':'sum'}).rename(columns={'pickups':'city_pickups'})\n",
    "            df = df.merge(df_city, on='datetime', how='left')\n",
    "\n",
    "            # city avg speed\n",
    "            df['city_avg_speed'] = df['city_pickups'] / df['city_trip_distance']\n",
    "\n",
    "            ## creating neighbor pickup counts\n",
    "            df_neigh = df.groupby('pulocationid').agg({'pickups':'sum'}).rename(columns={'pickups':'neighbor_pickups_sum'})\n",
    "            df = df.merge(df_neigh, on='pulocationid', how='left')\n",
    "\n",
    "            ##  Final features needed for prediction\n",
    "               # 'city_avg_speed',\n",
    "              #  'zone_avg_speed',\n",
    "             #   'zone_congestion_index',\n",
    "            #    'pickups_lag_1h',\n",
    "           #     'pulocationid',\n",
    "          #      'pickups_lag_24h',\n",
    "         #       'city_pickups_lag_1h',\n",
    "           #     'neighbor_pickups_lag_1h'\n",
    "\n",
    "\n",
    "\n",
    "            # Generating lag features\n",
    "            df['pickup_lag1'] = df['pickups'].shift(1)\n",
    "            df['pickup_lag24'] = df['pickups'].shift(24)\n",
    "            \n",
    "            df.reset_index(inplace=True)\n",
    "\n",
    "            logger.info(\"Lag features for prediction generated successfully.\")\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"failed to engineer lag features for prediction data {e}\")\n",
    "            raise RideDemandException(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8786bb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "def get_nyc_prediction_weather_data() -> pd.DataFrame:\n",
    "    try:\n",
    "        api_key = \"\" \n",
    "        location = \"New York, NY, United States\"\n",
    "        base_url = \"https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline\"\n",
    "\n",
    "        params = {\n",
    "            \"unitGroup\": \"us\",\n",
    "            \"key\": api_key,\n",
    "            \"include\": \"current\",\n",
    "            \"contentType\": \"json\"\n",
    "        }\n",
    "\n",
    "        url = f\"{base_url}/{location}/today\"\n",
    "        print(f\"Fetching data for: {datetime.now().strftime('%Y-%m-%d %H')}:00:00\")\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        # Extract current conditions\n",
    "        current = data.get(\"currentConditions\")\n",
    "        if not current:\n",
    "            print(\"No current conditions found in response.\")\n",
    "            return None\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        df_hours = pd.DataFrame([current]) \n",
    "        \n",
    "        # Select columns - check if they exist to prevent KeyErrors\n",
    "        cols = ['datetime', 'temp', 'humidity']\n",
    "        df_hours = df_hours[cols]\n",
    "\n",
    "        # FIX: Ensure datetime includes today's date so localization works correctly\n",
    "        today_date = datetime.now().strftime('%Y-%m-%d')\n",
    "        df_hours['datetime'] = pd.to_datetime(today_date + ' ' + df_hours['datetime'])\n",
    "        \n",
    "        # Localize and Floor\n",
    "        df_hours['datetime'] = df_hours['datetime'].dt.tz_localize('UTC').dt.floor('h')\n",
    "        df_hours.rename(columns={'datetime': 'bin'}, inplace=True)\n",
    "\n",
    "        print(f\"Successfully retrieved weather data.\")\n",
    "        return df_hours\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract weather data: {e}\")\n",
    "        return None # Return None instead of raising to prevent Colab from hanging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-19 16:01:11,896 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2026-01-19 16:01:11,901 INFO: Initializing external client\n",
      "2026-01-19 16:01:11,905 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2026-01-19 16:01:17,128 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1257642\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (145.72s) \n",
      "Successfully retrieved 6240 rows for window: 2025-11-30 00:00:00 to 2025-12-01 00:00:00\n",
      "2026-01-19 16:03:55,527 INFO: Fetching data for current timestamp 2026-01-19, 12:00:00 ...\n",
      "2026-01-19 16:03:57,193 INFO: Successfully Retrieved the weather data for 2026-01-19, 12:00:00.\n",
      "2026-01-19 16:03:57,196 INFO: Temporal Feature for prediction generated successfully.\n",
      "(1, 5) (6240, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin</th>\n",
       "      <th>temp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>pickup_hour</th>\n",
       "      <th>is_rush_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2026-01-19 12:00:00</td>\n",
       "      <td>29.9</td>\n",
       "      <td>61.3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  bin  temp  humidity  pickup_hour  is_rush_hour\n",
       "0 2026-01-19 12:00:00  29.9      61.3           12             0"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = extract_historical_pickup_data(start_date, end_date)\n",
    "df = get_nyc_prediction_weather_data()\n",
    "complete = engineer_temporal_prediction_features(df)\n",
    "print(df.shape, data.shape)\n",
    "complete.head()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f5c37bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pulocationid</th>\n",
       "      <th>bin</th>\n",
       "      <th>temp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>pickup_hour</th>\n",
       "      <th>is_rush_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68</td>\n",
       "      <td>2026-01-19 12:00:00</td>\n",
       "      <td>29.9</td>\n",
       "      <td>61.3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63</td>\n",
       "      <td>2026-01-19 12:00:00</td>\n",
       "      <td>29.9</td>\n",
       "      <td>61.3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109</td>\n",
       "      <td>2026-01-19 12:00:00</td>\n",
       "      <td>29.9</td>\n",
       "      <td>61.3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>168</td>\n",
       "      <td>2026-01-19 12:00:00</td>\n",
       "      <td>29.9</td>\n",
       "      <td>61.3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>242</td>\n",
       "      <td>2026-01-19 12:00:00</td>\n",
       "      <td>29.9</td>\n",
       "      <td>61.3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pulocationid                 bin  temp  humidity  pickup_hour  is_rush_hour\n",
       "0            68 2026-01-19 12:00:00  29.9      61.3           12             0\n",
       "1            63 2026-01-19 12:00:00  29.9      61.3           12             0\n",
       "2           109 2026-01-19 12:00:00  29.9      61.3           12             0\n",
       "3           168 2026-01-19 12:00:00  29.9      61.3           12             0\n",
       "4           242 2026-01-19 12:00:00  29.9      61.3           12             0"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_pulocationids = pd.DataFrame({'pulocationid': data.pulocationid.unique()})\n",
    "complete = pd.merge(unique_pulocationids, complete, how='cross')\n",
    "complete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bf047982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6500, 16)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.concat([data, complete], axis=0, ignore_index=True)\n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b3130244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>pickup_hour</th>\n",
       "      <th>is_rush_hour</th>\n",
       "      <th>city_avg_speed</th>\n",
       "      <th>zone_avg_speed</th>\n",
       "      <th>zone_congestion_index</th>\n",
       "      <th>pickups_lag_1h</th>\n",
       "      <th>pulocationid</th>\n",
       "      <th>pickups_lag_24h</th>\n",
       "      <th>city_pickups_lag_1h</th>\n",
       "      <th>neighbor_pickups_lag_1h</th>\n",
       "      <th>neighbor_pickups_sum</th>\n",
       "      <th>pickups</th>\n",
       "      <th>bin_str</th>\n",
       "      <th>bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>29.9</td>\n",
       "      <td>61.3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2026-01-19 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>29.9</td>\n",
       "      <td>61.3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>198</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2026-01-19 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6497</th>\n",
       "      <td>29.9</td>\n",
       "      <td>61.3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>164</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2026-01-19 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6498</th>\n",
       "      <td>29.9</td>\n",
       "      <td>61.3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>166</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2026-01-19 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6499</th>\n",
       "      <td>29.9</td>\n",
       "      <td>61.3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2026-01-19 12:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      temp  humidity  pickup_hour  is_rush_hour  city_avg_speed  \\\n",
       "6495  29.9      61.3           12             0             NaN   \n",
       "6496  29.9      61.3           12             0             NaN   \n",
       "6497  29.9      61.3           12             0             NaN   \n",
       "6498  29.9      61.3           12             0             NaN   \n",
       "6499  29.9      61.3           12             0             NaN   \n",
       "\n",
       "      zone_avg_speed  zone_congestion_index  pickups_lag_1h  pulocationid  \\\n",
       "6495             NaN                    NaN             NaN           108   \n",
       "6496             NaN                    NaN             NaN           198   \n",
       "6497             NaN                    NaN             NaN           164   \n",
       "6498             NaN                    NaN             NaN           166   \n",
       "6499             NaN                    NaN             NaN           143   \n",
       "\n",
       "      pickups_lag_24h  city_pickups_lag_1h  neighbor_pickups_lag_1h  \\\n",
       "6495              NaN                  NaN                      NaN   \n",
       "6496              NaN                  NaN                      NaN   \n",
       "6497              NaN                  NaN                      NaN   \n",
       "6498              NaN                  NaN                      NaN   \n",
       "6499              NaN                  NaN                      NaN   \n",
       "\n",
       "      neighbor_pickups_sum  pickups bin_str                  bin  \n",
       "6495                   NaN      NaN     NaN  2026-01-19 12:00:00  \n",
       "6496                   NaN      NaN     NaN  2026-01-19 12:00:00  \n",
       "6497                   NaN      NaN     NaN  2026-01-19 12:00:00  \n",
       "6498                   NaN      NaN     NaN  2026-01-19 12:00:00  \n",
       "6499                   NaN      NaN     NaN  2026-01-19 12:00:00  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a77b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.sort_values(by=['pulocationid','bin'], inplace=True)\n",
    "new_df.set_index(['bin', 'pulocationid'], inplace=True)\n",
    "new_df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4bdabe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4cb8f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Cache neighbor dictionary\n",
    "_neighbor_dict = None\n",
    "_neighbor_cache_path = os.path.join('artifacts/inference', \"neighbors.pkl\")\n",
    "\n",
    "def _get_neighbor_dict() -> dict:\n",
    "        global _neighbor_dict\n",
    "        if _neighbor_dict is not None:\n",
    "            return _neighbor_dict\n",
    "\n",
    "        if os.path.exists(_neighbor_cache_path):\n",
    "            try:\n",
    "                with open(_neighbor_cache_path, \"rb\") as f:\n",
    "                    _neighbor_dict = pickle.load(f)\n",
    "                print(\"Loaded neighbor dictionary from cache\")\n",
    "                return _neighbor_dict\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load neighbor cache: {e}\")\n",
    "\n",
    "        zones_gdf = load_shapefile_from_zip(config.taxi_zone_shapefile_url,\n",
    "                                            config.shapefile_dir)\n",
    "        zones_gdf_left = zones_gdf.rename(columns={\"LocationID\": \"LocationID_left\"})\n",
    "        zones_gdf_right = zones_gdf.rename(columns={\"LocationID\": \"LocationID_right\"})\n",
    "        neighbors_df = gpd.sjoin(zones_gdf_left, zones_gdf_right, how=\"left\", predicate=\"touches\")\n",
    "        neighbors_df = neighbors_df[neighbors_df['LocationID_left'] != neighbors_df['LocationID_right']]\n",
    "        _neighbor_dict = (neighbors_df.groupby('LocationID_left')['LocationID_right']\n",
    "                                .apply(lambda s: sorted(list(set(s))))\n",
    "                                .to_dict())\n",
    "        try:\n",
    "            os.makedirs(config.shapefile_dir, exist_ok=True)\n",
    "            with open(_neighbor_cache_path, \"wb\") as f:\n",
    "                pickle.dump(_neighbor_dict, f)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to persist neighbor cache: {e}\")\n",
    "        logger.info(\"neighbor dict retrieved successfully\")\n",
    "        return _neighbor_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04b76a4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_shapefile_from_zip' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43m_get_neighbor_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36m_get_neighbor_dict\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     17\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to load neighbor cache: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m zones_gdf = \u001b[43mload_shapefile_from_zip\u001b[49m(config.taxi_zone_shapefile_url,\n\u001b[32m     20\u001b[39m                                     config.shapefile_dir)\n\u001b[32m     21\u001b[39m zones_gdf_left = zones_gdf.rename(columns={\u001b[33m\"\u001b[39m\u001b[33mLocationID\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mLocationID_left\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m     22\u001b[39m zones_gdf_right = zones_gdf.rename(columns={\u001b[33m\"\u001b[39m\u001b[33mLocationID\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mLocationID_right\u001b[39m\u001b[33m\"\u001b[39m})\n",
      "\u001b[31mNameError\u001b[39m: name 'load_shapefile_from_zip' is not defined"
     ]
    }
   ],
   "source": [
    "_get_neighbor_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51c88808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2026-01-21 12:20:41,573 ]: DynamicPricingEngine: INFO: common_utils: 46: successfully load the yaml file from path: config/config.yaml\n",
      "[ 2026-01-21 12:20:41,662 ]: DynamicPricingEngine: INFO: common_utils: 46: successfully load the yaml file from path: params.yaml\n",
      "[ 2026-01-21 12:20:41,666 ]: DynamicPricingEngine: INFO: common_utils: 102: successfully created directory at: ['artifacts']\n",
      "[ 2026-01-21 12:20:41,667 ]: DynamicPricingEngine: INFO: configuration: 28: Artifacts root directory successfully created: artifacts\n",
      "[ 2026-01-21 12:20:41,670 ]: DynamicPricingEngine: INFO: common_utils: 102: successfully created directory at: ['artifacts/inference']\n",
      "[ 2026-01-21 12:20:41,671 ]: DynamicPricingEngine: INFO: configuration: 113: Inference root directory created successfully: artifacts/inference\n",
      "artifacts/inference/shapefile\n",
      "https://d37ci6vzurychx.cloudfront.net/misc/taxi_zones.zip\n"
     ]
    }
   ],
   "source": [
    "from src.DynamicPricingEngine.utils.common_utils import load_shapefile_from_zip\n",
    "from src.DynamicPricingEngine.config.configuration import ConfigurationManager\n",
    "config = ConfigurationManager()\n",
    "infconf= config.get_inference_config()\n",
    "\n",
    "print(infconf.shapefile_dir)\n",
    "print(infconf.taxi_zone_shapefile_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a3448b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bbabb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b463803b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2268227159.py, line 4)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mzone_df = pd.read_csv(https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv)#zone_lookup_url)\u001b[39m\n                               ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Load the official NYC Taxi Zone lookup table\n",
    "import pandas as pd\n",
    "zone_lookup_url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\"\n",
    "zone_df = pd.read_csv('https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv')#zone_lookup_url)\n",
    "\n",
    "print(zone_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f5e56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_shapefile_from_zip(url, extract_to):\n",
    "    # Step 1: Download the zipfile\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # raise error if download failed\n",
    "\n",
    "    # Step 2: Extract the zipfile\n",
    "    with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "        z.extractall(extract_to)\n",
    "\n",
    "    # Step 3: Find the .shp file (main shapefile component)\n",
    "    shp_file = None\n",
    "    for root, dirs, files in os.walk(extract_to):\n",
    "        for file in files:\n",
    "            if file.endswith(\".shp\"):\n",
    "                shp_file = os.path.join(root, file)\n",
    "                break\n",
    "\n",
    "    if shp_file is None:\n",
    "        raise FileNotFoundError(\"No .shp file found in the extracted archive.\")\n",
    "\n",
    "    # Step 4: Load shapefile into GeoDataFrame\n",
    "    gdf = gpd.read_file(shp_file)\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e72162e",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadZipFile\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m gdf = \u001b[43mload_shapefile_from_zip\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfconf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtaxi_zone_shapefile_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfconf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshapefile_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Dynamic-Pricing-Engine/src/DynamicPricingEngine/utils/common_utils.py:186\u001b[39m, in \u001b[36mload_shapefile_from_zip\u001b[39m\u001b[34m(url, extract_to)\u001b[39m\n\u001b[32m    183\u001b[39m response.raise_for_status()  \u001b[38;5;66;03m# raise error if download failed\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[38;5;66;03m# Extract the zipfile\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m z:\n\u001b[32m    187\u001b[39m     z.extractall(extract_to)\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# Find the .shp file (main shapefile component)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Dynamic-Pricing-Engine/venv/lib/python3.12/zipfile/__init__.py:1338\u001b[39m, in \u001b[36mZipFile.__init__\u001b[39m\u001b[34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[39m\n\u001b[32m   1336\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1337\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1338\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_RealGetContents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1339\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mx\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m   1340\u001b[39m         \u001b[38;5;66;03m# set the modified flag so central directory gets written\u001b[39;00m\n\u001b[32m   1341\u001b[39m         \u001b[38;5;66;03m# even if no files are added to the archive\u001b[39;00m\n\u001b[32m   1342\u001b[39m         \u001b[38;5;28mself\u001b[39m._didModify = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Dynamic-Pricing-Engine/venv/lib/python3.12/zipfile/__init__.py:1405\u001b[39m, in \u001b[36mZipFile._RealGetContents\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1403\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[33m\"\u001b[39m\u001b[33mFile is not a zip file\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m endrec:\n\u001b[32m-> \u001b[39m\u001b[32m1405\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[33m\"\u001b[39m\u001b[33mFile is not a zip file\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1406\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.debug > \u001b[32m1\u001b[39m:\n\u001b[32m   1407\u001b[39m     \u001b[38;5;28mprint\u001b[39m(endrec)\n",
      "\u001b[31mBadZipFile\u001b[39m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "#gdf = load_shapefile_from_zip(infconf.taxi_zone_shapefile_url, infconf.shapefile_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "852c51f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b''\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(infconf.taxi_zone_shapefile_url)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d43385",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadZipFile\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m zones_gdf = \u001b[43mload_shapefile_from_zip\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfconf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtaxi_zone_shapefile_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m                                        \u001b[49m\u001b[43minfconf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshapefile_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Dynamic-Pricing-Engine/src/DynamicPricingEngine/utils/common_utils.py:186\u001b[39m, in \u001b[36mload_shapefile_from_zip\u001b[39m\u001b[34m(url, extract_to)\u001b[39m\n\u001b[32m    183\u001b[39m response.raise_for_status()  \u001b[38;5;66;03m# raise error if download failed\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[38;5;66;03m# Extract the zipfile\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m z:\n\u001b[32m    187\u001b[39m     z.extractall(extract_to)\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# Find the .shp file (main shapefile component)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Dynamic-Pricing-Engine/venv/lib/python3.12/zipfile/__init__.py:1338\u001b[39m, in \u001b[36mZipFile.__init__\u001b[39m\u001b[34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[39m\n\u001b[32m   1336\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1337\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1338\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_RealGetContents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1339\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mx\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m   1340\u001b[39m         \u001b[38;5;66;03m# set the modified flag so central directory gets written\u001b[39;00m\n\u001b[32m   1341\u001b[39m         \u001b[38;5;66;03m# even if no files are added to the archive\u001b[39;00m\n\u001b[32m   1342\u001b[39m         \u001b[38;5;28mself\u001b[39m._didModify = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Dynamic-Pricing-Engine/venv/lib/python3.12/zipfile/__init__.py:1405\u001b[39m, in \u001b[36mZipFile._RealGetContents\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1403\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[33m\"\u001b[39m\u001b[33mFile is not a zip file\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m endrec:\n\u001b[32m-> \u001b[39m\u001b[32m1405\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[33m\"\u001b[39m\u001b[33mFile is not a zip file\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1406\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.debug > \u001b[32m1\u001b[39m:\n\u001b[32m   1407\u001b[39m     \u001b[38;5;28mprint\u001b[39m(endrec)\n",
      "\u001b[31mBadZipFile\u001b[39m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "#zones_gdf = load_shapefile_from_zip(infconf.taxi_zone_shapefile_url,\n",
    "                 #                       infconf.shapefile_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d7e07b",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Cache neighbor dictionary\n",
    "_neighbor_dict = None\n",
    "_neighbor_cache_path = os.path.join('artifacts/inference', \"neighbors.pkl\")\n",
    "\n",
    "def _get_neighbor_dict() -> dict:\n",
    "    global _neighbor_dict  # Declare it as global\n",
    "    if _neighbor_dict is not None:\n",
    "        return _neighbor_dict\n",
    "\n",
    "    if os.path.exists(_neighbor_cache_path):\n",
    "        try:\n",
    "            with open(_neighbor_cache_path, \"rb\") as f:\n",
    "                _neighbor_dict = pickle.load(f)\n",
    "            print(\"Loaded neighbor dictionary from cache\")\n",
    "            return _neighbor_dict\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load neighbor cache: {e}\")\n",
    "\n",
    "    zones_gdf = load_shapefile_from_zip(taxi_zone_shapefile_url,\n",
    "                                        shapefile_dir)\n",
    "    zones_gdf_left = zones_gdf.rename(columns={\"LocationID\": \"LocationID_left\"})\n",
    "    zones_gdf_right = zones_gdf.rename(columns={\"LocationID\": \"LocationID_right\"})\n",
    "    neighbors_df = gpd.sjoin(zones_gdf_left, zones_gdf_right, how=\"left\", predicate=\"touches\")\n",
    "    neighbors_df = neighbors_df[neighbors_df['LocationID_left'] != neighbors_df['LocationID_right']]\n",
    "    _neighbor_dict = (neighbors_df.groupby('LocationID_left')['LocationID_right']\n",
    "                            .apply(lambda s: sorted(list(set(s))))\n",
    "                            .to_dict())\n",
    "    try:\n",
    "        os.makedirs(shapefile_dir, exist_ok=True)\n",
    "        with open(_neighbor_cache_path, \"wb\") as f:\n",
    "            pickle.dump(_neighbor_dict, f)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to persist neighbor cache: {e}\")\n",
    "    return _neighbor_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
